{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import time\n",
        "import os\n",
        "import csv"
      ],
      "metadata": {
        "id": "XwGM36jtcK-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## !!! WE ARE NOT USING THIS !!!\n",
        "def blind_extraction(text):\n",
        "  for line in text:\n",
        "    if line == \"\\n\":\n",
        "      newLine = True\n",
        "      count +=1\n",
        "      if count ==2:\n",
        "          count = 0\n",
        "          print(list_1)\n",
        "          extracted_abstracts.append({\n",
        "              'PMID': list_1[5],\n",
        "              'doi': list_1[4],\n",
        "              'title': list_1[1],\n",
        "              'authors': list_1[2],\n",
        "              'text': list_1[3],\n",
        "          })\n",
        "          list_1 = []\n",
        "    else:\n",
        "        lineData = line\n",
        "        skip_append_flag = False\n",
        "\n",
        "        # Extract DOI as a seperate item\n",
        "        if(lineData.startswith(\"DOI:\")):\n",
        "          lineData = lineData.replace(\"DOI:\", \"\")\n",
        "\n",
        "        #Extract PMID as a seperate item\n",
        "        if(lineData.startswith(\"PMID:\")):\n",
        "          lineData = lineData.replace(\"PMID:\", \"\")\n",
        "          lineData = lineData.replace(\"[Indexed for MEDLINE]\", \"\")\n",
        "          list_1.append(lineData.rstrip().lstrip())\n",
        "          skip_append_flag = True\n",
        "\n",
        "        if(lineData.startswith(\"Copyright\")):\n",
        "          skip_append_flag = True\n",
        "\n",
        "        if(lineData.startswith(\"Author information:\")):\n",
        "          skip_append_flag = True\n",
        "        # Extract publication name --- future work\n",
        "        # Extract date published --- future work\n",
        "\n",
        "        if(not skip_append_flag):\n",
        "          if(newLine==False):\n",
        "            lineData = list_1[-1] +\" \"+ lineData.rstrip().lstrip()\n",
        "            list_1[-1] = lineData\n",
        "          else:\n",
        "            list_1.append(lineData.rstrip().lstrip())\n",
        "\n",
        "        count = 0\n",
        "        newLine=False"
      ],
      "metadata": {
        "id": "puZSC_7hq7KU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abstract_data_dataframe = pd.DataFrame(columns=['PMID','doi', 'title', 'authors', 'text'])"
      ],
      "metadata": {
        "id": "5NCZieJVXiLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def checkCSVFile(file_name):\n",
        "  if not os.path.isfile(file_name):\n",
        "    # If the file doesn't exist, create it\n",
        "    with open(file_name, 'w', newline='') as csvfile:\n",
        "        # Create a CSV writer object\n",
        "        csv_writer = csv.writer(csvfile)\n",
        "\n",
        "    print(f\"CSV file '{file_name}' has been created.\")\n",
        "  else:\n",
        "      print(f\"CSV file '{file_name}' already exists.\")"
      ],
      "metadata": {
        "id": "NoJnEGL5LYnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def writeFailedPmid(pmid):\n",
        "  failed_pmid_filename = \"./failedPmids.csv\"\n",
        "  #writing to csv file\n",
        "  with open(filename, 'a') as csvfile:\n",
        "    #creating a csv writer object\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "\n",
        "    # writing the data rows\n",
        "    csvwriter.writerows([[pmid]])"
      ],
      "metadata": {
        "id": "toHAIYWrVLbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def writeDataToCSVFile(data, abstract_filename):\n",
        "  # name of csv file\n",
        "  filename = abstract_filename\n",
        "  #writing to csv file\n",
        "  with open(filename, 'a') as csvfile:\n",
        "    #creating a csv writer object\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "\n",
        "    # writing the data rows\n",
        "    csvwriter.writerows([[data[\"pmid\"], data[\"doi\"], data[\"authors\"], data[\"title\"], data[\"text\"]]])\n",
        "\n"
      ],
      "metadata": {
        "id": "Tj9gDwrDAFeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recoverDataFromXML(XML_data):\n",
        "  extracted_data = {\n",
        "      \"pmid\":\"\",\n",
        "      \"doi\":\"\",\n",
        "      \"authors\":\"\",\n",
        "      \"title\":\"\",\n",
        "      \"text\":\"\"\n",
        "  }\n",
        "  tree = ET.ElementTree(ET.fromstring(XML_data.replace('\\n', '')))\n",
        "  for elem in tree.iter():\n",
        "    if(elem.tag == \"ArticleTitle\"):\n",
        "      extracted_data[\"title\"] = elem.text\n",
        "    if(elem.tag == \"ArticleId\"):\n",
        "      if(elem.attrib[\"IdType\"] == \"doi\"):\n",
        "        extracted_data[\"doi\"] = elem.text\n",
        "    if(elem.tag == \"AbstractText\"):\n",
        "      extracted_data[\"text\"] = elem.text\n",
        "    if(elem.tag == \"AuthorList\"):\n",
        "      author_list = []\n",
        "      for child in elem:\n",
        "        if(child.find(\"ForeName\") is not None and child.find(\"LastName\") is not None):\n",
        "          author_name=str(getattr(child.find(\"ForeName\"),\"text\"))+\" \"+str(getattr(child.find(\"LastName\"),\"text\"))\n",
        "          author_list.append(author_name)\n",
        "      extracted_data[\"authors\"] = author_list\n",
        "\n",
        "  return extracted_data"
      ],
      "metadata": {
        "id": "Ggh--Cx50yRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getAbstractData(pmID,filename, testResponseMode = False, count = 0) :\n",
        "  # api-endpoint\n",
        "  URL = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id={pmID}&retmode=XML&rettype=abstract&api_key=eb7ce0fc2436e350ccef78a21c6b1a494508\"\n",
        "\n",
        "  # sending get request and saving the response as response object\n",
        "  r = requests.get(url = URL)\n",
        "  print(r)\n",
        "  extractedText = None\n",
        "  # only for debug mode\n",
        "  if(testResponseMode):\n",
        "    print(r.text)\n",
        "  # write data to CSV when API response is 200\n",
        "  elif (testResponseMode==False and r.status_code == 200):\n",
        "    extractedText =  recoverDataFromXML(r.text)\n",
        "    extractedText[\"pmid\"] = pmID\n",
        "    writeDataToCSVFile(extractedText, filename)\n",
        "  #if API request failed (mostly failes due to excess load, wait a second and retry until count hits 5)\n",
        "  elif (testResponseMode==False and r.status_code != 200 and count<=5):\n",
        "    time.sleep(1)\n",
        "    count = count + 1\n",
        "    getAbstractData(pmID,filename,False,count)\n",
        "\n",
        "  return True"
      ],
      "metadata": {
        "id": "CaRBT8Mbx5MO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getAbstractData(35801624, True)"
      ],
      "metadata": {
        "id": "myomPSqXOimj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLh6-onjRU_5"
      },
      "outputs": [],
      "source": [
        "def abstract_data_main_function(i):\n",
        "  print(i)\n",
        "  logFile = open(\"./extraction_log.txt\",\"w\")\n",
        "  dataFile = open(\"./completionRecord.txt\",\"a\")\n",
        "  dataFile.write(f\"extracted_text_{i}.csv ----- STARTED \\n\")\n",
        "  csv_abstract_filename = f\"/content/drive/MyDrive/NLPT(Abstract Data)/extracted_csv_data/extracted_text_{i}.csv\"\n",
        "  checkCSVFile(csv_abstract_filename)\n",
        "  file = open(f\"raw_pmid_data/abstract-intelligen-set ({i}).txt\", 'r', encoding=\"utf-8\")\n",
        "  logFile.write(f\" working on file abstracts-original/abstract-intelligen-set ({i}).txt \\n\")\n",
        "  print(f\"abstracts-original/abstract-intelligen-set ({i}).txt\")\n",
        "  text = file.readlines()\n",
        "  count = 0\n",
        "  pmid_extracted_counter = 0\n",
        "  api_rate_limiter = 0\n",
        "  pmid_val = \"\"\n",
        "  chunk_abstracts = []\n",
        "  newLine = True\n",
        "  for line in text:\n",
        "    if line == \"\\n\":\n",
        "      newLine = True\n",
        "      count +=1\n",
        "      if count ==2:\n",
        "        count = 0\n",
        "        if(pmid_val != \"\"):\n",
        "          pmid_extracted_counter += 1\n",
        "          if(pmid_extracted_counter >= 0):\n",
        "            print(pmid_extracted_counter)\n",
        "            api_rate_limiter += 1\n",
        "            logFile.write(f\" pmid_extracted_counter: ({pmid_extracted_counter}) \\n\")\n",
        "            getAbstractData(pmid_val,csv_abstract_filename,False)\n",
        "            print(\"RATE LIMITER: \"+ str(api_rate_limiter))\n",
        "            if(api_rate_limiter >= 9):\n",
        "                print(\"RATE LIMIT\")\n",
        "                time.sleep(1)\n",
        "                api_rate_limiter = 0\n",
        "          #abstract_data_dataframe = pd.concat([abstract_data_dataframe, pd.DataFrame(extracted_text)], axis=0, ignore_index=True)\n",
        "        list_1 = []\n",
        "    else:\n",
        "      count = 0\n",
        "      lineData = line\n",
        "      #Extract PMID as a seperate item\n",
        "      if(lineData.startswith(\"PMID:\")):\n",
        "        lineData = lineData.replace(\"PMID:\", \"\")\n",
        "        lineData = lineData.replace(\"[Indexed for MEDLINE]\", \"\")\n",
        "        pmid_val = lineData.rstrip().lstrip()\n",
        "  dataFile.write(f\"extracted_text_{i}.csv ----- COMLPLETED \\n\")\n",
        "  return True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunk_abstracts)"
      ],
      "metadata": {
        "id": "S2xMkvK2wCiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cziQm74M51nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getPMIDs (i):\n",
        "  chunk_abstracts = []\n",
        "  file = open(f\"/content/drive/MyDrive/NLPT(Abstract Data)/abstracts-original/abstract-intelligen-set ({i}).txt\", 'r', encoding=\"utf-8\")\n",
        "  text = file.readlines()\n",
        "  count = 0\n",
        "  pmid_val = \"\"\n",
        "  chunk_abstracts = []\n",
        "  newLine = True\n",
        "  for line in text:\n",
        "    if line == \"\\n\":\n",
        "      newLine = True\n",
        "      count +=1\n",
        "      if count ==2:\n",
        "        count = 0\n",
        "        if(pmid_val != \"\"):\n",
        "          chunk_abstracts.append(pmid_val)\n",
        "          #abstract_data_dataframe = pd.concat([abstract_data_dataframe, pd.DataFrame(extracted_text)], axis=0, ignore_index=True)\n",
        "        list_1 = []\n",
        "    else:\n",
        "      count = 0\n",
        "      lineData = line\n",
        "      #Extract PMID as a seperate item\n",
        "      if(lineData.startswith(\"PMID:\")):\n",
        "        lineData = lineData.replace(\"PMID:\", \"\")\n",
        "        lineData = lineData.replace(\"[Indexed for MEDLINE]\", \"\")\n",
        "        pmid_val = lineData.rstrip().lstrip()\n",
        "  return chunk_abstracts\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iP_xF-nEATSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_abstracts = getPMIDs()"
      ],
      "metadata": {
        "id": "uI2HMg4_AplQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunk_abstracts)"
      ],
      "metadata": {
        "id": "oGzWfjttBVS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/NLPT(Abstract Data)/extracted_csv_data/extracted_text_8.csv')"
      ],
      "metadata": {
        "id": "k4lVJ8j3BrKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = ['PMID','doi', 'title', 'authors', 'text']"
      ],
      "metadata": {
        "id": "lGqPeNFCBvZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "iFy_qD7oCXus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "read_pmids = df[\"PMID\"].astype(str).tolist()"
      ],
      "metadata": {
        "id": "3W_wVlFJCaBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "read_pmids[1]"
      ],
      "metadata": {
        "id": "lUYTeKqlC8OG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_missing_elements(i):\n",
        "    arr1 = getPMIDs(i)\n",
        "    if(not os.path.exists(f'/content/drive/MyDrive/NLPT(Abstract Data)/data_backup/extracted_data/extracted_text_{i}.csv')):\n",
        "      print(f\"{i} : File missing: {len(arr1)}\")\n",
        "      return []\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(f'/content/drive/MyDrive/NLPT(Abstract Data)/data_backup/extracted_data/extracted_text_{i}.csv', on_bad_lines='skip')\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(f\"{i} : File empty: {len(arr1)}\")\n",
        "        return []\n",
        "\n",
        "    if(df.empty):\n",
        "      print(f\"{i} : File empty: {len(arr1)}\")\n",
        "      return []\n",
        "\n",
        "    df.columns = ['PMID','doi', 'title', 'authors', 'text']\n",
        "    read_pmids = df[\"PMID\"].astype(str).tolist()\n",
        "    missing_elements = []\n",
        "    # Convert arrays to sets for faster comparison\n",
        "    set1 = set(arr1)\n",
        "    set2 = set(read_pmids)\n",
        "\n",
        "    # Find elements in arr1 that are not in arr2\n",
        "    missing_elements = list(set1 - set2)\n",
        "    print(f\"{i} : Missing elements: {len(missing_elements)}\")\n",
        "    return missing_elements"
      ],
      "metadata": {
        "id": "5mbs_srUCii5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_missing_elements(43)"
      ],
      "metadata": {
        "id": "bpYr_9m1bHCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tempDict = {}\n",
        "for i in range (1,52):\n",
        "  tempDict[i] = find_missing_elements(i)"
      ],
      "metadata": {
        "id": "EnoETd_L6M38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_missing_elements(43)"
      ],
      "metadata": {
        "id": "ZI_bZSmbP3BX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tempDict[2]"
      ],
      "metadata": {
        "id": "YtBtNyt6QmOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missed_pmids = tempDict[42]"
      ],
      "metadata": {
        "id": "5lacd8oZCwrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(missed_pmids)"
      ],
      "metadata": {
        "id": "FuIMGLB6C467"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def addMissedPmidsToCSV(i):\n",
        "  missed_pmids = find_missing_elements(i)\n",
        "  csv_abstract_filename = f\"/content/drive/MyDrive/NLPT(Abstract Data)/data_backup/extracted_data/extracted_text_{i}.csv\"\n",
        "  api_rate_limiter = 0\n",
        "  for i in range(0, len(missed_pmids)):\n",
        "    print(i)\n",
        "    extracted_text = getAbstractData(missed_pmids[i],csv_abstract_filename)\n",
        "    api_rate_limiter += 1\n",
        "    # if(api_rate_limiter == 9):\n",
        "    #   time.sleep(1)\n",
        "    #   api_rate_limiter = 0"
      ],
      "metadata": {
        "id": "LN051swDDRFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for id in tempDict:\n",
        "  if(len(tempDict[id])>0):\n",
        "    addMissedPmidsToCSV(id)"
      ],
      "metadata": {
        "id": "1YT5MzKlNeX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "addMissedPmidsToCSV(6)"
      ],
      "metadata": {
        "id": "IZk7EBmaEOML"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}