# -*- coding: utf-8 -*-
"""data_extraction_converted.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xpHid38-_n-Z0n9OoN4ywC63tp7Faj2c
"""

import pandas as pd
import re
import requests
import xml.etree.ElementTree as ET
import time
import os
import csv

## !!! WE ARE NOT USING THIS !!!
def blind_extraction(text):
  for line in text:
    if line == "\n":
      newLine = True
      count +=1
      if count ==2:
          count = 0
          print(list_1)
          extracted_abstracts.append({
              'PMID': list_1[5],
              'doi': list_1[4],
              'title': list_1[1],
              'authors': list_1[2],
              'text': list_1[3],
          })
          list_1 = []
    else:
        lineData = line
        skip_append_flag = False

        # Extract DOI as a seperate item
        if(lineData.startswith("DOI:")):
          lineData = lineData.replace("DOI:", "")

        #Extract PMID as a seperate item
        if(lineData.startswith("PMID:")):
          lineData = lineData.replace("PMID:", "")
          lineData = lineData.replace("[Indexed for MEDLINE]", "")
          list_1.append(lineData.rstrip().lstrip())
          skip_append_flag = True

        if(lineData.startswith("Copyright")):
          skip_append_flag = True

        if(lineData.startswith("Author information:")):
          skip_append_flag = True
        # Extract publication name --- future work
        # Extract date published --- future work

        if(not skip_append_flag):
          if(newLine==False):
            lineData = list_1[-1] +" "+ lineData.rstrip().lstrip()
            list_1[-1] = lineData
          else:
            list_1.append(lineData.rstrip().lstrip())

        count = 0
        newLine=False

abstract_data_dataframe = pd.DataFrame(columns=['PMID','doi', 'title', 'authors', 'text'])

def checkCSVFile(file_name):
  if not os.path.isfile(file_name):
    # If the file doesn't exist, create it
    with open(file_name, 'w', newline='') as csvfile:
        # Create a CSV writer object
        csv_writer = csv.writer(csvfile)

    print(f"CSV file '{file_name}' has been created.")
  else:
      print(f"CSV file '{file_name}' already exists.")

global csv_abstract_filename

checkCSVFile("./failedPmids.csv")

# checkCSVFile(csv_abstract_filename)

def writeFailedPmid(pmid):
  failed_pmid_filename = "./failedPmids.csv"
  #writing to csv file
  with open(filename, 'a') as csvfile:
    #creating a csv writer object
    csvwriter = csv.writer(csvfile)

    # writing the data rows
    csvwriter.writerows([[pmid]])

def writeDataToCSVFile(data, abstract_filename):
  # name of csv file
  filename = abstract_filename
  #writing to csv file
  with open(filename, 'a') as csvfile:
    #creating a csv writer object
    csvwriter = csv.writer(csvfile)

    # writing the data rows
    csvwriter.writerows([[data["pmid"], data["doi"], data["authors"], data["title"], data["text"]]])

def recoverDataFromXML(XML_data):
  extracted_data = {
      "pmid":"",
      "doi":"",
      "authors":"",
      "title":"",
      "text":""
  }
  tree = ET.ElementTree(ET.fromstring(XML_data.replace('\n', '')))
  for elem in tree.iter():
    if(elem.tag == "ArticleTitle"):
      extracted_data["title"] = elem.text
    if(elem.tag == "ArticleId"):
      if(elem.attrib["IdType"] == "doi"):
        extracted_data["doi"] = elem.text
    if(elem.tag == "AbstractText"):
      extracted_data["text"] = elem.text
    if(elem.tag == "AuthorList"):
      author_list = []
      for child in elem:
        if(child.find("ForeName") != None and child.find("LastName") != None):
          author_name=getattr(child.find("ForeName"),"text")+" "+getattr(child.find("LastName"),"text")
          author_list.append(author_name)
      extracted_data["authors"] = author_list

  return extracted_data

def getAbstractData(pmID,filename, testResponseMode = False) :
  # api-endpoint
  URL = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id={pmID}&retmode=XML&rettype=abstract&api_key=b55c0812eec1fcf4062d95d31443a333df08"

  # sending get request and saving the response as response object
  r = requests.get(url = URL)
  print(r)
  extractedText = None
  if(testResponseMode):
    print(r.text)
  elif (testResponseMode==False and r.status_code == 200):
    extractedText =  recoverDataFromXML(r.text)
    extractedText["pmid"] = pmID
    writeDataToCSVFile(extractedText, filename)
  elif (testResponseMode==False and r.status_code != 200):
    writeFailedPmid(pmID)
  return True

def abstract_data_main_function(i):
  print(i)
  logFile = open("./extraction_log.txt","w")
  csv_abstract_filename = f"ex_data/extracted_text_{i}.csv"
  checkCSVFile(csv_abstract_filename)
  file = open(f"raw_pmid_data/abstract-intelligen-set ({i}).txt", 'r', encoding="utf-8")
  logFile.write(f" working on file abstracts-original/abstract-intelligen-set ({i}).txt \n")
  text = file.readlines()
  count = 0
  pmid_extracted_counter = 0
  api_rate_limiter = 0
  pmid_val = ""
  chunk_abstracts = []
  newLine = True
  for line in text:
    if line == "\n":
      newLine = True
      count +=1
      if count ==2:
        count = 0
        if(pmid_val != ""):
          pmid_extracted_counter += 1
          if(pmid_extracted_counter >= 0):
            print(pmid_extracted_counter)
            api_rate_limiter += 1
            logFile.write(f" pmid_extracted_counter: ({pmid_extracted_counter}) \n")
            getAbstractData(pmid_val,csv_abstract_filename,False)
            if(api_rate_limiter == 3):
              time.sleep(1)
              api_rate_limiter = 0
          #abstract_data_dataframe = pd.concat([abstract_data_dataframe, pd.DataFrame(extracted_text)], axis=0, ignore_index=True)
        list_1 = []
    else:
      count = 0
      lineData = line
      #Extract PMID as a seperate item
      if(lineData.startswith("PMID:")):
        lineData = lineData.replace("PMID:", "")
        lineData = lineData.replace("[Indexed for MEDLINE]", "")
        pmid_val = lineData.rstrip().lstrip()
  return True

# abstract_data_main_function(6)

# len(chunk_abstracts)

def getPMIDs ():
  chunk_abstracts = []
  for i in range(0,1):
      file = open(f"/content/drive/MyDrive/NLPT(Abstract Data)/abstracts-original/abstract-intelligen-set (4).txt", 'r', encoding="utf-8")
      text = file.readlines()
      count = 0
      pmid_val = ""
      chunk_abstracts = []
      newLine = True
      for line in text:
        if line == "\n":
          newLine = True
          count +=1
          if count ==2:
            count = 0
            if(pmid_val != ""):
              chunk_abstracts.append(pmid_val)
              #abstract_data_dataframe = pd.concat([abstract_data_dataframe, pd.DataFrame(extracted_text)], axis=0, ignore_index=True)
            list_1 = []
        else:
          count = 0
          lineData = line
          #Extract PMID as a seperate item
          if(lineData.startswith("PMID:")):
            lineData = lineData.replace("PMID:", "")
            lineData = lineData.replace("[Indexed for MEDLINE]", "")
            pmid_val = lineData.rstrip().lstrip()
  return chunk_abstracts

# chunk_abstracts = getPMIDs()

# len(chunk_abstracts)

# df = pd.read_csv('/content/drive/MyDrive/NLPT(Abstract Data)/extracted_text_4.csv')

# df.columns = ['PMID','doi', 'title', 'authors', 'text']

# df.head()

# read_pmids = df["PMID"].astype(str).tolist()

# read_pmids[1]

def find_missing_elements(arr1, arr2):
    missing_elements = []
    # Convert arrays to sets for faster comparison
    set1 = set(arr1)
    set2 = set(arr2)

    # Find elements in arr1 that are not in arr2
    missing_elements = list(set1 - set2)

    return missing_elements

# missed_pmids = find_missing_elements(chunk_abstracts,read_pmids)

# len(missed_pmids)

def addMissedPmidsToCSV(missed_pmids):
  api_rate_limiter = 0
  for i in range(0, len(missed_pmids)):
    print(i)
    extracted_text = getAbstractData(missed_pmids[i])
    api_rate_limiter += 1
    if(api_rate_limiter == 3):
      time.sleep(1)
      api_rate_limiter = 0

# addMissedPmidsToCSV(missed_pmids)