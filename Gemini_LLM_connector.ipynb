{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t65FnCJ-QzB8"
      },
      "outputs": [],
      "source": [
        "%pip install sentence_transformers -q\n",
        "%pip install langchain -q\n",
        "%pip install langchain-openai -q\n",
        "%pip install google-cloud-aiplatform -q\n",
        "%pip install opensearch-py -q\n",
        "%pip install gradio -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "tiZYfXspTQBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "from opensearchpy import OpenSearch, helpers\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "z9as2OyERx5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.preview.generative_models import GenerativeModel, HarmCategory, HarmBlockThreshold\n",
        "from google.cloud import aiplatform\n",
        "import vertexai"
      ],
      "metadata": {
        "id": "TpmCyv51TiZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This authentication is only for colab\n",
        "# If you are setting this up locally on your mac or AWS, you simply install gcloud cli and then run `gcloud auth login` one time on your terminal.\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth as google_auth\n",
        "\n",
        "    google_auth.authenticate_user()"
      ],
      "metadata": {
        "id": "SAVeoFFSTTK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenSearch(\n",
        "   hosts=[\"https://admin:2NCbjLJWWzIFw@ec2-34-207-194-37.compute-1.amazonaws.com:9200/\"],\n",
        "    http_compress=True,\n",
        "    use_ssl=True,\n",
        "    verify_certs=False,\n",
        "    ssl_assert_hostname=False,\n",
        "    ssl_show_warn=False,\n",
        ")"
      ],
      "metadata": {
        "id": "7A62qwIjR6aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INDEX_NAME = \"inlpt-without-chunking\"\n",
        "INDEX_NAME = \"inlpt-without-title-chunking\"\n",
        "# INDEX_NAME = \"nitish-test\""
      ],
      "metadata": {
        "id": "defe_rWqTIYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding model\n",
        "model_miniLM = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "LpuC9DwbTL8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'gemini-1.0-pro'\n",
        "project_id = 'inlpt-gen-ai-416111' # TODO: fill this\n",
        "location = 'us-central1'\n",
        "generation_config = {\n",
        "    'max_output_tokens': 2048,\n",
        "    'temperature': 0.2,\n",
        "    'top_p': 0.85,\n",
        "    'top_k': 40\n",
        "}"
      ],
      "metadata": {
        "id": "dhFkXPSJTcMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise client\n",
        "vertexai.init(project=project_id, location=location)\n",
        "model = GenerativeModel(model_name, generation_config=generation_config)"
      ],
      "metadata": {
        "id": "mthRpEvbTmxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt template\n",
        "prompt = '''You are an expert on life sciences and biomedical topics.\n",
        "\n",
        "Research Information:\n",
        "{context}\n",
        "\n",
        "- Use only the GIVEN research information above and answer the user question. Try to justify your answer using the research information and support with examples (attribute authors if present) from the context.\n",
        "- Respond with the answer.\n",
        "- If you cannot find the answer to the user question, ask user to rephrase or provide more context.\n",
        "\n",
        "User question:\n",
        "{question}\n",
        "\n",
        "Response:\n",
        "'''"
      ],
      "metadata": {
        "id": "vioIh_OFTpp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'Has Augmented intelligence become the focus of clinical interest?'"
      ],
      "metadata": {
        "id": "hbhLr_dJTtYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Are there developmental delays in children with ADHD\""
      ],
      "metadata": {
        "id": "Hmc1pCEITwOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Is the psychological profile of GERD patients, especially those without hiatal hernia, considered in the evaluation of clinical symptoms and outcomes of antireflux surgery? [Yes or No]\""
      ],
      "metadata": {
        "id": "vOQ5oavScLpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collate search results to a single string\n",
        "def get_info(query: str):\n",
        "\n",
        "  # Create search query\n",
        "  query = {\n",
        "      \"size\": 3,\n",
        "      \"query\": {\"knn\": {\"embedding\": {\"vector\": model_miniLM.encode(query), \"k\": 10}}},\n",
        "      \"_source\": False,\n",
        "      \"fields\": [\"id\",\"doi\",\"authors\", \"text\"],\n",
        "  }\n",
        "\n",
        "  results = client.search(body=query, index=INDEX_NAME)\n",
        "\n",
        "  results = results['hits']['hits']\n",
        "\n",
        "  context = \"\"\n",
        "  for row in results:\n",
        "    value = row['fields']['text'][0] + \"\\n\"\n",
        "    value += f\"AUTHORS: {row['fields']['authors'][0]}\\n\"\n",
        "\n",
        "    context += value + \"\\n\" + \"- - - - - \"*10 + \"\\n\"\n",
        "\n",
        "  return context"
      ],
      "metadata": {
        "id": "_jugAFD1UJXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradio_retr_query(history, question, limit=3):\n",
        "  historical_conv = \"\"\n",
        "  print(\"HISTORY:\",history[-limit:])\n",
        "  for message in history[-limit:]:\n",
        "    historical_conv += f\"User said: {message[0]}\\n\"\n",
        "    historical_conv += f\"Model said: {message[1]}\\n\"\n",
        "    historical_conv += \"* * * * * * \"*10\n",
        "    historical_conv += \"\\n\"\n",
        "\n",
        "  response = model.generate_content([f'''Using the historical conversation below, check if the user question is a follow-up question.\n",
        "\n",
        "Historical conversation:\n",
        "{historical_conv}\n",
        "\n",
        "User question: {question}\n",
        "\n",
        "- If yes, it is follow-up question\n",
        "  - Modify and complete the user question to use it as a query for information retrieval\n",
        "  - Replace pronouns where possible.\n",
        "  - Try not to change meaning of the question.\n",
        "- If no, it is not a follow-up question\n",
        "  - Do not modify user question. Respond with ONLY \"{question}\".\n",
        "\n",
        "Respond with only the final retrieval query.\n",
        "Query for information retrieval:'''])\n",
        "\n",
        "  output = question\n",
        "\n",
        "  try:\n",
        "    output = response.text\n",
        "\n",
        "  except Exception as e:\n",
        "    print(\"Error generating response.\")\n",
        "    print(e)\n",
        "\n",
        "  return output\n"
      ],
      "metadata": {
        "id": "W3UHh5cMQVMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Safety config\n",
        "safety_config = {\n",
        "      HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "      HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "      HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "      HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "      HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE\n",
        "}"
      ],
      "metadata": {
        "id": "bYB8UHbAlXou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gemini_response(question, history):\n",
        "    if len(history):\n",
        "      search_query = gradio_retr_query(history, question)\n",
        "    else:\n",
        "      search_query = question\n",
        "\n",
        "    print(\"SEARCH QUERY:\", search_query)\n",
        "    context = get_info(search_query)\n",
        "\n",
        "    print(prompt.format(\n",
        "      context = context,\n",
        "      question = search_query\n",
        "    ))\n",
        "\n",
        "    response = model.generate_content([prompt.format(\n",
        "      context = context,\n",
        "      question = question\n",
        "      )], safety_settings=safety_config)\n",
        "\n",
        "    output = \"Please rephrase or ask another questions.\"\n",
        "\n",
        "    try:\n",
        "      output = response.text\n",
        "\n",
        "    except Exception as e:\n",
        "      print(\"Error generating response.\")\n",
        "      print(e)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "pqbkE_7mQD9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "pZTBRAWoSGKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox()\n",
        "    clear = gr.ClearButton([msg, chatbot])\n",
        "\n",
        "    def respond(message, chat_history):\n",
        "        bot_message = get_gemini_response(message, chat_history)\n",
        "        chat_history.append((message, bot_message))\n",
        "\n",
        "        return \"\", chat_history\n",
        "\n",
        "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "xQcF4oD9VMML"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}