[{"context": "Micro and nanobots (MNBs) are unprecedented in their ability to be chemically tuned for autonomous tasks with enhanced targeting and functionality while maintaining their mobility. A myriad of chemical modifications involving a large variety of advanced materials have been demonstrated to be effective in the design of MNBs. Furthermore, they can be controlled for their autonomous motion, and their ability to carry chemical or biological payloads. In addition, MNBs can be modified to achieve targetability with specificity for biological implications. MNBs by virtue of their chemical compositions may be limited by their biocompatibility, tissue accumulation, poor biodegradability and toxicity. This review presents a note on artificial intelligence materials (AIMs), their importance, and the dimensional scales at which intrinsic autonomy can be achieved for diverse utility. We briefly discuss the evolution of such systems with a focus on their advancements in nanomedicine. We highlight MNBs covering their contemporary traits and the emergence of a few start-ups in specific areas. Furthermore, we showcase various examples, demonstrating that chemical tunability is an attractive primary approach for designing MNBs with immense capabilities both in biology and chemistry. Finally, we cover biosafety and ethical considerations in designing MNBs in the era of artificial intelligence for varied applications.", "type": "CONFIRMATIONAL", "question": "QUESTION: Can MNBs be modified to achieve targetability with specificity for biological implications?\nANSWER: Yes"}, {"context": "Micro and nanobots (MNBs) are unprecedented in their ability to be chemically tuned for autonomous tasks with enhanced targeting and functionality while maintaining their mobility. A myriad of chemical modifications involving a large variety of advanced materials have been demonstrated to be effective in the design of MNBs. Furthermore, they can be controlled for their autonomous motion, and their ability to carry chemical or biological payloads. In addition, MNBs can be modified to achieve targetability with specificity for biological implications. MNBs by virtue of their chemical compositions may be limited by their biocompatibility, tissue accumulation, poor biodegradability and toxicity. This review presents a note on artificial intelligence materials (AIMs), their importance, and the dimensional scales at which intrinsic autonomy can be achieved for diverse utility. We briefly discuss the evolution of such systems with a focus on their advancements in nanomedicine. We highlight MNBs covering their contemporary traits and the emergence of a few start-ups in specific areas. Furthermore, we showcase various examples, demonstrating that chemical tunability is an attractive primary approach for designing MNBs with immense capabilities both in biology and chemistry. Finally, we cover biosafety and ethical considerations in designing MNBs in the era of artificial intelligence for varied applications.", "type": "FACTOID", "question": "QUESTION : Which chemical modifications have been shown to be effective in the design of MNBs?\nCONTEXT : A myriad of chemical modifications involving a large variety of advanced materials have been demonstrated to be effective in the design of MNBs."}, {"context": "Micro and nanobots (MNBs) are unprecedented in their ability to be chemically tuned for autonomous tasks with enhanced targeting and functionality while maintaining their mobility. A myriad of chemical modifications involving a large variety of advanced materials have been demonstrated to be effective in the design of MNBs. Furthermore, they can be controlled for their autonomous motion, and their ability to carry chemical or biological payloads. In addition, MNBs can be modified to achieve targetability with specificity for biological implications. MNBs by virtue of their chemical compositions may be limited by their biocompatibility, tissue accumulation, poor biodegradability and toxicity. This review presents a note on artificial intelligence materials (AIMs), their importance, and the dimensional scales at which intrinsic autonomy can be achieved for diverse utility. We briefly discuss the evolution of such systems with a focus on their advancements in nanomedicine. We highlight MNBs covering their contemporary traits and the emergence of a few start-ups in specific areas. Furthermore, we showcase various examples, demonstrating that chemical tunability is an attractive primary approach for designing MNBs with immense capabilities both in biology and chemistry. Finally, we cover biosafety and ethical considerations in designing MNBs in the era of artificial intelligence for varied applications.", "type": "CASUAL", "question": "**CONTEXT :** MNBs by virtue of their chemical compositions may be limited by their biocompatibility, tissue accumulation, poor biodegradability and toxicity.\n\n**QUESTION :** Why are MNBs limited in their applications due to their chemical compositions?"}, {"context": "['We determined whether the human capability for sensemaking, or for identifying essential elements of information (EEIs), could be enhanced by a simulated recognition aid that directed attention to people and vehicles in scenes or by a simulated recognition aid that directed attention to EEIs.', \"For intelligence analysts, sensemaking is challenging because it frequently involves making inferences about uncertain data. One way to enhance sensemaking may involve collaboration from a machine recognition aid such as Project Maven, an established algorithm that directs analysts' attention to people and vehicles in scenes. We simulated the directed attention of Project Maven as well as a machine recognition aid that directed attention to EEIs.\", \"We created full-motion videos of simulated compounds viewed by an overhead camera. Sensemaking was assessed by measuring participants' ability to predict events and identify signs. Participants' attention was directed by placing small globe symbols above either all people and vehicles, or all EEIs. Novices and intelligence analysts participated.\", \"Simulated recognition aids directing participants' attention to EEIs improved EEI identification but directing attention to people and vehicles (emulating Project Maven) did not. Participants' sensemaking was not enhanced by either type of simulated recognition aid.\", 'Guiding attention to features in a scene improves their identification whereas indiscriminate steering of attention to entities in the scene does not improve understanding of the holistic meaning of events, unless attention is drawn to relevant signs of those events.', 'Results contribute to our goal of determining which human-machine systems improve the sensemaking capability of intelligence analysts in the field.']", "type": "CONFIRMATIONAL", "question": "QUESTION: Did the simulated recognition aid that directed attention to people and vehicles (emulating Project Maven) enhance participants' sensemaking?\nANSWER: No"}, {"context": "['We determined whether the human capability for sensemaking, or for identifying essential elements of information (EEIs), could be enhanced by a simulated recognition aid that directed attention to people and vehicles in scenes or by a simulated recognition aid that directed attention to EEIs.', \"For intelligence analysts, sensemaking is challenging because it frequently involves making inferences about uncertain data. One way to enhance sensemaking may involve collaboration from a machine recognition aid such as Project Maven, an established algorithm that directs analysts' attention to people and vehicles in scenes. We simulated the directed attention of Project Maven as well as a machine recognition aid that directed attention to EEIs.\", \"We created full-motion videos of simulated compounds viewed by an overhead camera. Sensemaking was assessed by measuring participants' ability to predict events and identify signs. Participants' attention was directed by placing small globe symbols above either all people and vehicles, or all EEIs. Novices and intelligence analysts participated.\", \"Simulated recognition aids directing participants' attention to EEIs improved EEI identification but directing attention to people and vehicles (emulating Project Maven) did not. Participants' sensemaking was not enhanced by either type of simulated recognition aid.\", 'Guiding attention to features in a scene improves their identification whereas indiscriminate steering of attention to entities in the scene does not improve understanding of the holistic meaning of events, unless attention is drawn to relevant signs of those events.', 'Results contribute to our goal of determining which human-machine systems improve the sensemaking capability of intelligence analysts in the field.']", "type": "FACTOID", "question": "**QUESTION :** Which simulated recognition aid was found to improve EEI identification?\n**CONTEXT :** Simulated recognition aids directing participants' attention to EEIs improved EEI identification but directing attention to people and vehicles (emulating Project Maven) did not."}, {"context": "['We determined whether the human capability for sensemaking, or for identifying essential elements of information (EEIs), could be enhanced by a simulated recognition aid that directed attention to people and vehicles in scenes or by a simulated recognition aid that directed attention to EEIs.', \"For intelligence analysts, sensemaking is challenging because it frequently involves making inferences about uncertain data. One way to enhance sensemaking may involve collaboration from a machine recognition aid such as Project Maven, an established algorithm that directs analysts' attention to people and vehicles in scenes. We simulated the directed attention of Project Maven as well as a machine recognition aid that directed attention to EEIs.\", \"We created full-motion videos of simulated compounds viewed by an overhead camera. Sensemaking was assessed by measuring participants' ability to predict events and identify signs. Participants' attention was directed by placing small globe symbols above either all people and vehicles, or all EEIs. Novices and intelligence analysts participated.\", \"Simulated recognition aids directing participants' attention to EEIs improved EEI identification but directing attention to people and vehicles (emulating Project Maven) did not. Participants' sensemaking was not enhanced by either type of simulated recognition aid.\", 'Guiding attention to features in a scene improves their identification whereas indiscriminate steering of attention to entities in the scene does not improve understanding of the holistic meaning of events, unless attention is drawn to relevant signs of those events.', 'Results contribute to our goal of determining which human-machine systems improve the sensemaking capability of intelligence analysts in the field.']", "type": "CASUAL", "question": "CONTEXT : \"Simulated recognition aids directing participants' attention to EEIs improved EEI identification but directing attention to people and vehicles (emulating Project Maven) did not.\"\nQUESTION : Why did directing attention to people and vehicles not improve EEI identification?"}, {"context": "Recent applications of dynamic network analyses to functional neuroimaging data have revealed relationships between a number of cognition conditions and the dynamic reconfiguration of brain networks. Here we critically review such applications of network neuroscience to intelligence. After providing an overview of network neuroscience, we center our discussion around the recently proposed Network Neuroscience Theory of Intelligence (Barbey, 2017). We evaluate and review existing empirical support for the theses made by this theory and argue that while studies strongly suggest their plausibility, evidence to date has largely been indirect. We propose avenues for future research to directly evaluate these theses by overcoming the methodological and analytical shortcomings of previous studies. In doing so, our goal is to stimulate future empirical investigations and present valuable ways forward in the network neuroscience of intelligence.", "type": "CONFIRMATIONAL", "question": "QUESTION: Does the Network Neuroscience Theory of Intelligence have direct empirical support?\nANSWER: No"}, {"context": "Recent applications of dynamic network analyses to functional neuroimaging data have revealed relationships between a number of cognition conditions and the dynamic reconfiguration of brain networks. Here we critically review such applications of network neuroscience to intelligence. After providing an overview of network neuroscience, we center our discussion around the recently proposed Network Neuroscience Theory of Intelligence (Barbey, 2017). We evaluate and review existing empirical support for the theses made by this theory and argue that while studies strongly suggest their plausibility, evidence to date has largely been indirect. We propose avenues for future research to directly evaluate these theses by overcoming the methodological and analytical shortcomings of previous studies. In doing so, our goal is to stimulate future empirical investigations and present valuable ways forward in the network neuroscience of intelligence.", "type": "FACTOID", "question": "**QUESTION :** Who proposed the Network Neuroscience Theory of Intelligence?\n\n**CONTEXT :** We center our discussion around the recently proposed Network Neuroscience Theory of Intelligence (Barbey, 2017)."}, {"context": "Recent applications of dynamic network analyses to functional neuroimaging data have revealed relationships between a number of cognition conditions and the dynamic reconfiguration of brain networks. Here we critically review such applications of network neuroscience to intelligence. After providing an overview of network neuroscience, we center our discussion around the recently proposed Network Neuroscience Theory of Intelligence (Barbey, 2017). We evaluate and review existing empirical support for the theses made by this theory and argue that while studies strongly suggest their plausibility, evidence to date has largely been indirect. We propose avenues for future research to directly evaluate these theses by overcoming the methodological and analytical shortcomings of previous studies. In doing so, our goal is to stimulate future empirical investigations and present valuable ways forward in the network neuroscience of intelligence.", "type": "CASUAL", "question": "**QUESTION :** Why has evidence for the Network Neuroscience Theory of Intelligence been largely indirect to date?\n\n**CONTEXT :** We evaluate and review existing empirical support for the theses made by this theory and argue that while studies strongly suggest their plausibility, evidence to date has largely been indirect."}, {"context": "The aim of this study was to investigate the feasibility of using machine learning techniques based on morphological features in classifying two subtypes of primary intestinal T-cell lymphomas (PITLs) defined according to the WHO criteria: monomorphic epitheliotropic intestinal T-cell lymphoma (MEITL) versus intestinal T-cell lymphoma, not otherwise specified (ITCL-NOS), which is considered a major challenge for pathological diagnosis. A total of 40 histopathological whole-slide images (WSIs) from 40 surgically resected PITL cases were used as the dataset for model training and testing. A deep neural network was trained to detect and segment the nuclei of lymphocytes. Quantitative nuclear morphometrics were further computed from these predicted contours. A decision-tree-based machine learning algorithm, XGBoost, was then trained to classify PITL cases into two disease subtypes using these nuclear morphometric features. The deep neural network achieved an average precision of 0.881 in the cell segmentation work. In terms of classifying MEITL versus ITCL-NOS, the XGBoost model achieved an area under receiver operating characteristic curve (AUC) of 0.966. Our research demonstrated an accurate, human-interpretable approach to using machine learning algorithms for reducing the high dimensionality of image features and classifying T cell lymphomas that present challenges in morphologic diagnosis. The quantitative nuclear morphometric features may lead to further discoveries concerning the relationship between cellular phenotype and disease status.", "type": "CONFIRMATIONAL", "question": "QUESTION: Did the XGBoost model achieve an AUC of 0.966 in classifying MEITL versus ITCL-NOS?\nANSWER: Yes"}, {"context": "The aim of this study was to investigate the feasibility of using machine learning techniques based on morphological features in classifying two subtypes of primary intestinal T-cell lymphomas (PITLs) defined according to the WHO criteria: monomorphic epitheliotropic intestinal T-cell lymphoma (MEITL) versus intestinal T-cell lymphoma, not otherwise specified (ITCL-NOS), which is considered a major challenge for pathological diagnosis. A total of 40 histopathological whole-slide images (WSIs) from 40 surgically resected PITL cases were used as the dataset for model training and testing. A deep neural network was trained to detect and segment the nuclei of lymphocytes. Quantitative nuclear morphometrics were further computed from these predicted contours. A decision-tree-based machine learning algorithm, XGBoost, was then trained to classify PITL cases into two disease subtypes using these nuclear morphometric features. The deep neural network achieved an average precision of 0.881 in the cell segmentation work. In terms of classifying MEITL versus ITCL-NOS, the XGBoost model achieved an area under receiver operating characteristic curve (AUC) of 0.966. Our research demonstrated an accurate, human-interpretable approach to using machine learning algorithms for reducing the high dimensionality of image features and classifying T cell lymphomas that present challenges in morphologic diagnosis. The quantitative nuclear morphometric features may lead to further discoveries concerning the relationship between cellular phenotype and disease status.", "type": "FACTOID", "question": "QUESTION : Which machine learning algorithm was used to classify PITL cases into two disease subtypes?\nCONTEXT : A decision-tree-based machine learning algorithm, XGBoost, was then trained to classify PITL cases into two disease subtypes using these nuclear morphometric features."}, {"context": "The aim of this study was to investigate the feasibility of using machine learning techniques based on morphological features in classifying two subtypes of primary intestinal T-cell lymphomas (PITLs) defined according to the WHO criteria: monomorphic epitheliotropic intestinal T-cell lymphoma (MEITL) versus intestinal T-cell lymphoma, not otherwise specified (ITCL-NOS), which is considered a major challenge for pathological diagnosis. A total of 40 histopathological whole-slide images (WSIs) from 40 surgically resected PITL cases were used as the dataset for model training and testing. A deep neural network was trained to detect and segment the nuclei of lymphocytes. Quantitative nuclear morphometrics were further computed from these predicted contours. A decision-tree-based machine learning algorithm, XGBoost, was then trained to classify PITL cases into two disease subtypes using these nuclear morphometric features. The deep neural network achieved an average precision of 0.881 in the cell segmentation work. In terms of classifying MEITL versus ITCL-NOS, the XGBoost model achieved an area under receiver operating characteristic curve (AUC) of 0.966. Our research demonstrated an accurate, human-interpretable approach to using machine learning algorithms for reducing the high dimensionality of image features and classifying T cell lymphomas that present challenges in morphologic diagnosis. The quantitative nuclear morphometric features may lead to further discoveries concerning the relationship between cellular phenotype and disease status.", "type": "CASUAL", "question": "**CONTEXT :** A deep neural network was trained to detect and segment the nuclei of lymphocytes. Quantitative nuclear morphometrics were further computed from these predicted contours.\n\n**QUESTION :** How were the quantitative nuclear morphometric features computed?"}, {"context": "Driven by recent advances in Artificial Intelligence (AI) and Computer Vision (CV), the implementation of AI systems in the medical domain increased correspondingly. This is especially true for the domain of medical imaging, in which the incorporation of AI aids several imaging-based tasks such as classification, segmentation, and registration. Moreover, AI reshapes medical research and contributes to the development of personalized clinical care. Consequently, alongside its extended implementation arises the need for an extensive understanding of AI systems and their inner workings, potentials, and limitations which the field of eXplainable AI (XAI) aims at. Because medical imaging is mainly associated with visual tasks, most explainability approaches incorporate saliency-based XAI methods. In contrast to that, in this article we would like to investigate the full potential of XAI methods in the field of medical imaging by specifically focusing on XAI techniques not relying on saliency, and providing diversified examples. We dedicate our investigation to a broad audience, but particularly healthcare professionals. Moreover, this work aims at establishing a common ground for cross-disciplinary understanding and exchange across disciplines between Deep Learning (DL) builders and healthcare professionals, which is why we aimed for a non-technical overview. Presented XAI methods are divided by a method's output representation into the following categories: Case-based explanations, textual explanations, and auxiliary explanations.", "type": "CONFIRMATIONAL", "question": "QUESTION: Does the article focus on saliency-based XAI methods in medical imaging?\nANSWER: No"}, {"context": "Driven by recent advances in Artificial Intelligence (AI) and Computer Vision (CV), the implementation of AI systems in the medical domain increased correspondingly. This is especially true for the domain of medical imaging, in which the incorporation of AI aids several imaging-based tasks such as classification, segmentation, and registration. Moreover, AI reshapes medical research and contributes to the development of personalized clinical care. Consequently, alongside its extended implementation arises the need for an extensive understanding of AI systems and their inner workings, potentials, and limitations which the field of eXplainable AI (XAI) aims at. Because medical imaging is mainly associated with visual tasks, most explainability approaches incorporate saliency-based XAI methods. In contrast to that, in this article we would like to investigate the full potential of XAI methods in the field of medical imaging by specifically focusing on XAI techniques not relying on saliency, and providing diversified examples. We dedicate our investigation to a broad audience, but particularly healthcare professionals. Moreover, this work aims at establishing a common ground for cross-disciplinary understanding and exchange across disciplines between Deep Learning (DL) builders and healthcare professionals, which is why we aimed for a non-technical overview. Presented XAI methods are divided by a method's output representation into the following categories: Case-based explanations, textual explanations, and auxiliary explanations.", "type": "FACTOID", "question": "**QUESTION :** Which type of XAI methods are incorporated in most explainability approaches for medical imaging?\n\n**CONTEXT :** Because medical imaging is mainly associated with visual tasks, most explainability approaches incorporate saliency-based XAI methods."}, {"context": "Driven by recent advances in Artificial Intelligence (AI) and Computer Vision (CV), the implementation of AI systems in the medical domain increased correspondingly. This is especially true for the domain of medical imaging, in which the incorporation of AI aids several imaging-based tasks such as classification, segmentation, and registration. Moreover, AI reshapes medical research and contributes to the development of personalized clinical care. Consequently, alongside its extended implementation arises the need for an extensive understanding of AI systems and their inner workings, potentials, and limitations which the field of eXplainable AI (XAI) aims at. Because medical imaging is mainly associated with visual tasks, most explainability approaches incorporate saliency-based XAI methods. In contrast to that, in this article we would like to investigate the full potential of XAI methods in the field of medical imaging by specifically focusing on XAI techniques not relying on saliency, and providing diversified examples. We dedicate our investigation to a broad audience, but particularly healthcare professionals. Moreover, this work aims at establishing a common ground for cross-disciplinary understanding and exchange across disciplines between Deep Learning (DL) builders and healthcare professionals, which is why we aimed for a non-technical overview. Presented XAI methods are divided by a method's output representation into the following categories: Case-based explanations, textual explanations, and auxiliary explanations.", "type": "CASUAL", "question": "**CONTEXT :** Driven by recent advances in Artificial Intelligence (AI) and Computer Vision (CV), the implementation of AI systems in the medical domain increased correspondingly.\n\n**QUESTION :** Why has the implementation of AI systems in the medical domain increased?"}, {"context": "In this paper, we propose an assisted driving system implemented with a Jetson nano-high-performance embedded platform by using machine vision and deep learning technologies. The vehicle dynamics model is established under multiconditional assumptions, the path planner and path tracking controller are designed based on the model predictive control algorithm, and the local desired path is reasonably planned in combination with the behavioral decision system. The behavioral decision algorithm based on finite state machine reasonably transforms the driving state according to the environmental changes, realizes the following of the target vehicle speed, and can take effective emergency braking in time when there is a collision danger. The system can complete the motion planning by the model predictive control algorithm and control the autonomous vehicle to smoothly track the replanned local desired path to complete the lane change overtaking action, which can meet the demand of ADAS. The path planner is designed based on the MPC algorithm, solving the objective function with obstacle avoidance function, planning the optimal path that can avoid a collision, and using 5th order polynomial to fit the output local desired path points. In 5\u223c8 s time, the target vehicle decelerates to 48 km/h; the autonomous vehicle immediately makes a deceleration action and gradually reduces the speed difference between the two vehicles until it reaches the target speed, at which time the distance between the two vehicles is close to the safe distance, obtained by the simulation test results. The system can still accurately track the target when the vehicle is driving on a curve and timely control the desired speed change of the vehicle, and the target vehicle always maintains a safe distance. The system can be used within 50 meters.", "type": "CONFIRMATIONAL", "question": "QUESTION: Does the system use a finite state machine for behavioral decision-making?\nANSWER: Yes"}, {"context": "In this paper, we propose an assisted driving system implemented with a Jetson nano-high-performance embedded platform by using machine vision and deep learning technologies. The vehicle dynamics model is established under multiconditional assumptions, the path planner and path tracking controller are designed based on the model predictive control algorithm, and the local desired path is reasonably planned in combination with the behavioral decision system. The behavioral decision algorithm based on finite state machine reasonably transforms the driving state according to the environmental changes, realizes the following of the target vehicle speed, and can take effective emergency braking in time when there is a collision danger. The system can complete the motion planning by the model predictive control algorithm and control the autonomous vehicle to smoothly track the replanned local desired path to complete the lane change overtaking action, which can meet the demand of ADAS. The path planner is designed based on the MPC algorithm, solving the objective function with obstacle avoidance function, planning the optimal path that can avoid a collision, and using 5th order polynomial to fit the output local desired path points. In 5\u223c8 s time, the target vehicle decelerates to 48 km/h; the autonomous vehicle immediately makes a deceleration action and gradually reduces the speed difference between the two vehicles until it reaches the target speed, at which time the distance between the two vehicles is close to the safe distance, obtained by the simulation test results. The system can still accurately track the target when the vehicle is driving on a curve and timely control the desired speed change of the vehicle, and the target vehicle always maintains a safe distance. The system can be used within 50 meters.", "type": "FACTOID", "question": "**QUESTION :** Which algorithm is used to design the path planner and path tracking controller?\n**CONTEXT :** The path planner and path tracking controller are designed based on the model predictive control algorithm"}, {"context": "In this paper, we propose an assisted driving system implemented with a Jetson nano-high-performance embedded platform by using machine vision and deep learning technologies. The vehicle dynamics model is established under multiconditional assumptions, the path planner and path tracking controller are designed based on the model predictive control algorithm, and the local desired path is reasonably planned in combination with the behavioral decision system. The behavioral decision algorithm based on finite state machine reasonably transforms the driving state according to the environmental changes, realizes the following of the target vehicle speed, and can take effective emergency braking in time when there is a collision danger. The system can complete the motion planning by the model predictive control algorithm and control the autonomous vehicle to smoothly track the replanned local desired path to complete the lane change overtaking action, which can meet the demand of ADAS. The path planner is designed based on the MPC algorithm, solving the objective function with obstacle avoidance function, planning the optimal path that can avoid a collision, and using 5th order polynomial to fit the output local desired path points. In 5\u223c8 s time, the target vehicle decelerates to 48 km/h; the autonomous vehicle immediately makes a deceleration action and gradually reduces the speed difference between the two vehicles until it reaches the target speed, at which time the distance between the two vehicles is close to the safe distance, obtained by the simulation test results. The system can still accurately track the target when the vehicle is driving on a curve and timely control the desired speed change of the vehicle, and the target vehicle always maintains a safe distance. The system can be used within 50 meters.", "type": "CASUAL", "question": "**CONTEXT :** The path planner is designed based on the MPC algorithm, solving the objective function with obstacle avoidance function, planning the optimal path that can avoid a collision, and using 5th order polynomial to fit the output local desired path points.\n\n**QUESTION :** Why is the path planner designed based on the MPC algorithm?"}, {"context": "['Lymphedema (LE) has been called the forgotten vascular disease, given such scant knowledge about LE-associated comorbidities or causes. Such knowledge of the comorbidities and treatment of LE may assist in diagnostic decisions and health care planning.', 'To determine the proportion of LE patients with various LE-associated comorbidities as well as the rate of associated treatment, deidentified Health Insurance Portability and Accountability Act-compliant commercial administrative claims from the Blue Health Intelligence (BHI) research database (165 million Blue Cross Blue Shield members) were queried. We analyzed a BHI study sample of 26,902 patients with LE who had been enrolled with continuous medical benefits for 12\\xa0months before and after the index date for the complete years 2012 through 2016. Patients were first identified by comorbidity and then grouped into those receiving no treatment for LE and those receiving any treatment for LE. Any treatment was defined as receiving manual lymphatic drainage, physical therapy, compression garments, or a pneumatic compression device. The purpose of this study was to determine the proportion of LE patients comorbid with various known LE-associated conditions and the treatment rates of LE patients with each comorbidity.', 'Among the 84,579,269 BHI patients enrolled during the study window, 81,366 patients were identified with LE. From this LE group, our study focused on the 26,902 patients who were enrolled with continuous medical and pharmacy benefits for 12\\xa0months before and after the index date. Among these 26,902 LE patients, breast cancer was the most frequent comorbidity with LE (32.1%), and these patients almost universally received any treatment (94.2%); other cancer types, such as melanoma (2.1%) and prostate cancer (0.7%), were less frequent and received any treatment less often, 75% and 82% of the time, respectively. Venous leg ulcer was the most common non-cancer-linked comorbidity for LE (9.6%), but only 81.7% of venous leg ulcer patients received any treatment for LE.', 'To our knowledge, this is the largest study to date detailing the comorbidities associated with LE and LE treatment rates within each. Our findings suggest that a sizable proportion of cancer-related LE patients do not receive appropriate treatment. Furthermore, this study highlights the role of advanced venous disease as an LE comorbidity that is frequently untreated and its associated gap in treatment.']", "type": "CONFIRMATIONAL", "question": "QUESTION: Did the study find that breast cancer was the most frequent comorbidity with lymphedema?\nANSWER: Yes"}, {"context": "['Lymphedema (LE) has been called the forgotten vascular disease, given such scant knowledge about LE-associated comorbidities or causes. Such knowledge of the comorbidities and treatment of LE may assist in diagnostic decisions and health care planning.', 'To determine the proportion of LE patients with various LE-associated comorbidities as well as the rate of associated treatment, deidentified Health Insurance Portability and Accountability Act-compliant commercial administrative claims from the Blue Health Intelligence (BHI) research database (165 million Blue Cross Blue Shield members) were queried. We analyzed a BHI study sample of 26,902 patients with LE who had been enrolled with continuous medical benefits for 12\\xa0months before and after the index date for the complete years 2012 through 2016. Patients were first identified by comorbidity and then grouped into those receiving no treatment for LE and those receiving any treatment for LE. Any treatment was defined as receiving manual lymphatic drainage, physical therapy, compression garments, or a pneumatic compression device. The purpose of this study was to determine the proportion of LE patients comorbid with various known LE-associated conditions and the treatment rates of LE patients with each comorbidity.', 'Among the 84,579,269 BHI patients enrolled during the study window, 81,366 patients were identified with LE. From this LE group, our study focused on the 26,902 patients who were enrolled with continuous medical and pharmacy benefits for 12\\xa0months before and after the index date. Among these 26,902 LE patients, breast cancer was the most frequent comorbidity with LE (32.1%), and these patients almost universally received any treatment (94.2%); other cancer types, such as melanoma (2.1%) and prostate cancer (0.7%), were less frequent and received any treatment less often, 75% and 82% of the time, respectively. Venous leg ulcer was the most common non-cancer-linked comorbidity for LE (9.6%), but only 81.7% of venous leg ulcer patients received any treatment for LE.', 'To our knowledge, this is the largest study to date detailing the comorbidities associated with LE and LE treatment rates within each. Our findings suggest that a sizable proportion of cancer-related LE patients do not receive appropriate treatment. Furthermore, this study highlights the role of advanced venous disease as an LE comorbidity that is frequently untreated and its associated gap in treatment.']", "type": "FACTOID", "question": "QUESTION : Which comorbidity was the most frequent among LE patients?\nCONTEXT : Among these 26,902 LE patients, breast cancer was the most frequent comorbidity with LE (32.1%), and these patients almost universally received any treatment (94.2%); other cancer types, such as melanoma (2.1%) and prostate cancer (0.7%), were less frequent and received any treatment less often, 75% and 82% of the time, respectively."}, {"context": "['Lymphedema (LE) has been called the forgotten vascular disease, given such scant knowledge about LE-associated comorbidities or causes. Such knowledge of the comorbidities and treatment of LE may assist in diagnostic decisions and health care planning.', 'To determine the proportion of LE patients with various LE-associated comorbidities as well as the rate of associated treatment, deidentified Health Insurance Portability and Accountability Act-compliant commercial administrative claims from the Blue Health Intelligence (BHI) research database (165 million Blue Cross Blue Shield members) were queried. We analyzed a BHI study sample of 26,902 patients with LE who had been enrolled with continuous medical benefits for 12\\xa0months before and after the index date for the complete years 2012 through 2016. Patients were first identified by comorbidity and then grouped into those receiving no treatment for LE and those receiving any treatment for LE. Any treatment was defined as receiving manual lymphatic drainage, physical therapy, compression garments, or a pneumatic compression device. The purpose of this study was to determine the proportion of LE patients comorbid with various known LE-associated conditions and the treatment rates of LE patients with each comorbidity.', 'Among the 84,579,269 BHI patients enrolled during the study window, 81,366 patients were identified with LE. From this LE group, our study focused on the 26,902 patients who were enrolled with continuous medical and pharmacy benefits for 12\\xa0months before and after the index date. Among these 26,902 LE patients, breast cancer was the most frequent comorbidity with LE (32.1%), and these patients almost universally received any treatment (94.2%); other cancer types, such as melanoma (2.1%) and prostate cancer (0.7%), were less frequent and received any treatment less often, 75% and 82% of the time, respectively. Venous leg ulcer was the most common non-cancer-linked comorbidity for LE (9.6%), but only 81.7% of venous leg ulcer patients received any treatment for LE.', 'To our knowledge, this is the largest study to date detailing the comorbidities associated with LE and LE treatment rates within each. Our findings suggest that a sizable proportion of cancer-related LE patients do not receive appropriate treatment. Furthermore, this study highlights the role of advanced venous disease as an LE comorbidity that is frequently untreated and its associated gap in treatment.']", "type": "CASUAL", "question": "CONTEXT : \"Our findings suggest that a sizable proportion of cancer-related LE patients do not receive appropriate treatment.\"\nQUESTION : Why do a sizable proportion of cancer-related LE patients not receive appropriate treatment?"}, {"context": "The year 2020 brought many changes to the lives of people all over the world with the outbreak of COVID-19; we saw lockdowns for months and deaths of many individuals, which set the world economy back miles. As research was conducted to create vaccines and cures that would eradicate the virus, precautionary measures were imposed on people to help reduce the spread the disease. These measures included washing of hands, appropriate distancing in social gatherings and wearing of masks to cover the face and nose. But due to human error, most people failed to adhere to this face mask rule and this could be monitored using artificial intelligence. In this work, we carried out a survey on Masked Face Recognition (MFR) and Occluded Face Recognition (OFR) deep learning techniques used to detect whether a face mask was being worn. The major problem faced by these models is that people often wear face masks incorrectly, either not covering the nose or mouth, which is equivalent to not wearing it at all. The deep learning algorithms detected the covered features on the face to ensure that the correct parts of the face were covered and had amazingly effective results.", "type": "CONFIRMATIONAL", "question": "QUESTION: Did the deep learning algorithms detect the covered features on the face to ensure that the correct parts of the face were covered?\nANSWER: Yes"}, {"context": "The year 2020 brought many changes to the lives of people all over the world with the outbreak of COVID-19; we saw lockdowns for months and deaths of many individuals, which set the world economy back miles. As research was conducted to create vaccines and cures that would eradicate the virus, precautionary measures were imposed on people to help reduce the spread the disease. These measures included washing of hands, appropriate distancing in social gatherings and wearing of masks to cover the face and nose. But due to human error, most people failed to adhere to this face mask rule and this could be monitored using artificial intelligence. In this work, we carried out a survey on Masked Face Recognition (MFR) and Occluded Face Recognition (OFR) deep learning techniques used to detect whether a face mask was being worn. The major problem faced by these models is that people often wear face masks incorrectly, either not covering the nose or mouth, which is equivalent to not wearing it at all. The deep learning algorithms detected the covered features on the face to ensure that the correct parts of the face were covered and had amazingly effective results.", "type": "FACTOID", "question": "**QUESTION :** Which deep learning techniques were used to detect whether a face mask was being worn?\n\n**CONTEXT :** In this work, we carried out a survey on Masked Face Recognition (MFR) and Occluded Face Recognition (OFR) deep learning techniques used to detect whether a face mask was being worn."}, {"context": "The year 2020 brought many changes to the lives of people all over the world with the outbreak of COVID-19; we saw lockdowns for months and deaths of many individuals, which set the world economy back miles. As research was conducted to create vaccines and cures that would eradicate the virus, precautionary measures were imposed on people to help reduce the spread the disease. These measures included washing of hands, appropriate distancing in social gatherings and wearing of masks to cover the face and nose. But due to human error, most people failed to adhere to this face mask rule and this could be monitored using artificial intelligence. In this work, we carried out a survey on Masked Face Recognition (MFR) and Occluded Face Recognition (OFR) deep learning techniques used to detect whether a face mask was being worn. The major problem faced by these models is that people often wear face masks incorrectly, either not covering the nose or mouth, which is equivalent to not wearing it at all. The deep learning algorithms detected the covered features on the face to ensure that the correct parts of the face were covered and had amazingly effective results.", "type": "CASUAL", "question": "**CONTEXT :** The major problem faced by these models is that people often wear face masks incorrectly, either not covering the nose or mouth, which is equivalent to not wearing it at all.\n\n**QUESTION :** Why is it important to wear face masks correctly?"}, {"context": "['Technical skill acquisition is an essential component of neurosurgical training. Educational theory suggests that optimal learning and improvement in performance depends on the provision of objective feedback. Therefore, the aim of this study was to develop a vision-based framework based on a novel representation of surgical tool motion and interactions capable of automated and objective assessment of microsurgical skill.', \"Videos were obtained from 1 expert, 6 intermediate, and 12 novice surgeons performing arachnoid dissection in a validated clinical model using a standard operating microscope. A mask region convolutional neural network framework was used to segment the tools present within the operative field in a recorded video frame. Tool motion analysis was achieved using novel triangulation metrics. Performance of the framework in classifying skill levels was evaluated using the area under the curve and accuracy. Objective measures of classifying the surgeons' skill level were also compared using the Mann-Whitney U test, and a value of P\\u2009\\u20090.05 was considered statistically significant.\", 'The area under the curve was 0.977 and the accuracy was 84.21%. A number of differences were found, which included experts having a lower median dissector velocity (P\\xa0= 0.0004; 190.38 ms[-1] vs. 116.38 ms[-1]), and a smaller inter-tool tip distance (median 46.78 vs. 75.92; P\\xa0=\\xa00.0002) compared with novices.', 'Automated and objective analysis of microsurgery is feasible using a mask region convolutional neural network, and a novel tool motion and interaction representation. This may support technical skills training and assessment in neurosurgery.']", "type": "CONFIRMATIONAL", "question": "QUESTION: Does the study suggest that technical skill acquisition is not an essential component of neurosurgical training?\nANSWER: No"}, {"context": "['Technical skill acquisition is an essential component of neurosurgical training. Educational theory suggests that optimal learning and improvement in performance depends on the provision of objective feedback. Therefore, the aim of this study was to develop a vision-based framework based on a novel representation of surgical tool motion and interactions capable of automated and objective assessment of microsurgical skill.', \"Videos were obtained from 1 expert, 6 intermediate, and 12 novice surgeons performing arachnoid dissection in a validated clinical model using a standard operating microscope. A mask region convolutional neural network framework was used to segment the tools present within the operative field in a recorded video frame. Tool motion analysis was achieved using novel triangulation metrics. Performance of the framework in classifying skill levels was evaluated using the area under the curve and accuracy. Objective measures of classifying the surgeons' skill level were also compared using the Mann-Whitney U test, and a value of P\\u2009\\u20090.05 was considered statistically significant.\", 'The area under the curve was 0.977 and the accuracy was 84.21%. A number of differences were found, which included experts having a lower median dissector velocity (P\\xa0= 0.0004; 190.38 ms[-1] vs. 116.38 ms[-1]), and a smaller inter-tool tip distance (median 46.78 vs. 75.92; P\\xa0=\\xa00.0002) compared with novices.', 'Automated and objective analysis of microsurgery is feasible using a mask region convolutional neural network, and a novel tool motion and interaction representation. This may support technical skills training and assessment in neurosurgery.']", "type": "FACTOID", "question": "CONTEXT : \"Videos were obtained from 1 expert, 6 intermediate, and 12 novice surgeons performing arachnoid dissection in a validated clinical model using a standard operating microscope.\"\nQUESTION : Which type of surgeons were involved in the study?"}, {"context": "['Technical skill acquisition is an essential component of neurosurgical training. Educational theory suggests that optimal learning and improvement in performance depends on the provision of objective feedback. Therefore, the aim of this study was to develop a vision-based framework based on a novel representation of surgical tool motion and interactions capable of automated and objective assessment of microsurgical skill.', \"Videos were obtained from 1 expert, 6 intermediate, and 12 novice surgeons performing arachnoid dissection in a validated clinical model using a standard operating microscope. A mask region convolutional neural network framework was used to segment the tools present within the operative field in a recorded video frame. Tool motion analysis was achieved using novel triangulation metrics. Performance of the framework in classifying skill levels was evaluated using the area under the curve and accuracy. Objective measures of classifying the surgeons' skill level were also compared using the Mann-Whitney U test, and a value of P\\u2009\\u20090.05 was considered statistically significant.\", 'The area under the curve was 0.977 and the accuracy was 84.21%. A number of differences were found, which included experts having a lower median dissector velocity (P\\xa0= 0.0004; 190.38 ms[-1] vs. 116.38 ms[-1]), and a smaller inter-tool tip distance (median 46.78 vs. 75.92; P\\xa0=\\xa00.0002) compared with novices.', 'Automated and objective analysis of microsurgery is feasible using a mask region convolutional neural network, and a novel tool motion and interaction representation. This may support technical skills training and assessment in neurosurgery.']", "type": "CASUAL", "question": "**CONTEXT :** \"Educational theory suggests that optimal learning and improvement in performance depends on the provision of objective feedback.\"\n\n**QUESTION :** Why is objective feedback important for optimal learning and performance improvement?"}, {"context": "In 1955, when John McCarthy and his colleagues proposed their first study of artificial intelligence, they suggested that 'every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it'. Whether that might ever be possible would depend on how we define intelligence, but what is indisputable is that new methods are needed to analyse and interpret the copious information provided by digital medical images, genomic databases, and biobanks. Technological advances have enabled applications of artificial intelligence (AI) including machine learning (ML) to be implemented into clinical practice, and their related scientific literature is exploding. Advocates argue enthusiastically that AI will transform many aspects of clinical cardiovascular medicine, while sceptics stress the importance of caution and the need for more evidence. This report summarizes the main opposing arguments that were presented in a debate at the 2021 Congress of the European Society of Cardiology. Artificial intelligence is an advanced analytical technique that should be considered when conventional statistical methods are insufficient, but testing a hypothesis or solving a clinical problem-not finding another application for AI-remains the most important objective. Artificial intelligence and ML methods should be transparent and interpretable, if they are to be approved by regulators and trusted to provide support for clinical decisions. Physicians need to understand AI methods and collaborate with engineers. Few applications have yet been shown to have a positive impact on clinical outcomes, so investment in research is essential.", "type": "CONFIRMATIONAL", "question": "QUESTION: Is artificial intelligence an advanced analytical technique that should be considered when conventional statistical methods are insufficient?\nANSWER: Yes"}, {"context": "In 1955, when John McCarthy and his colleagues proposed their first study of artificial intelligence, they suggested that 'every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it'. Whether that might ever be possible would depend on how we define intelligence, but what is indisputable is that new methods are needed to analyse and interpret the copious information provided by digital medical images, genomic databases, and biobanks. Technological advances have enabled applications of artificial intelligence (AI) including machine learning (ML) to be implemented into clinical practice, and their related scientific literature is exploding. Advocates argue enthusiastically that AI will transform many aspects of clinical cardiovascular medicine, while sceptics stress the importance of caution and the need for more evidence. This report summarizes the main opposing arguments that were presented in a debate at the 2021 Congress of the European Society of Cardiology. Artificial intelligence is an advanced analytical technique that should be considered when conventional statistical methods are insufficient, but testing a hypothesis or solving a clinical problem-not finding another application for AI-remains the most important objective. Artificial intelligence and ML methods should be transparent and interpretable, if they are to be approved by regulators and trusted to provide support for clinical decisions. Physicians need to understand AI methods and collaborate with engineers. Few applications have yet been shown to have a positive impact on clinical outcomes, so investment in research is essential.", "type": "FACTOID", "question": "QUESTION : When did John McCarthy and his colleagues propose their first study of artificial intelligence?\nCONTEXT : In 1955, when John McCarthy and his colleagues proposed their first study of artificial intelligence, they suggested that 'every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it'."}, {"context": "In 1955, when John McCarthy and his colleagues proposed their first study of artificial intelligence, they suggested that 'every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it'. Whether that might ever be possible would depend on how we define intelligence, but what is indisputable is that new methods are needed to analyse and interpret the copious information provided by digital medical images, genomic databases, and biobanks. Technological advances have enabled applications of artificial intelligence (AI) including machine learning (ML) to be implemented into clinical practice, and their related scientific literature is exploding. Advocates argue enthusiastically that AI will transform many aspects of clinical cardiovascular medicine, while sceptics stress the importance of caution and the need for more evidence. This report summarizes the main opposing arguments that were presented in a debate at the 2021 Congress of the European Society of Cardiology. Artificial intelligence is an advanced analytical technique that should be considered when conventional statistical methods are insufficient, but testing a hypothesis or solving a clinical problem-not finding another application for AI-remains the most important objective. Artificial intelligence and ML methods should be transparent and interpretable, if they are to be approved by regulators and trusted to provide support for clinical decisions. Physicians need to understand AI methods and collaborate with engineers. Few applications have yet been shown to have a positive impact on clinical outcomes, so investment in research is essential.", "type": "CASUAL", "question": "CONTEXT : Technological advances have enabled applications of artificial intelligence (AI) including machine learning (ML) to be implemented into clinical practice, and their related scientific literature is exploding.\n\nQUESTION : Why have technological advancements led to the implementation of AI and ML in clinical practice?"}, {"context": "This study develops a method to implement a quantum field lens coding and classification algorithm for two quantum double-field (QDF) system models: 1- a QDF model, and 2- a QDF lens coding model by a DF computation (DFC). This method determines entanglement entropy (EE) by implementing QDF operators in a quantum circuit. The physical link between the two system models is a quantum field lens coding algorithm (QF-LCA), which is a QF lens distance-based, implemented on real N -qubit machines. This is with the possibility to train the algorithm for making strong predictions on phase transitions as the shared objective of both models. In both system models, QDF transformations are simulated by a DFC algorithm where QDF data are collected and analyzed to represent energy states and transitions, and determine entanglement based on EE. The method gives a list of steps to simulate and optimize any thermodynamic system on macro and micro-scale observations, as presented in this article:\u2022The implementation of QF-LCA on quantum computers with EE measurement under a QDF transformation.\u2022Validation of QF-LCA as implemented compared to quantum Fourier transform (QFT) and its inverse, QFT - 1 .\u2022Quantum artificial intelligence (QAI) features by classifying QDF with strong measurement outcome predictions.", "type": "CONFIRMATIONAL", "question": "QUESTION: Does the study present a method for implementing a quantum field lens coding and classification algorithm for two quantum double-field (QDF) system models?\nANSWER: Yes"}, {"context": "This study develops a method to implement a quantum field lens coding and classification algorithm for two quantum double-field (QDF) system models: 1- a QDF model, and 2- a QDF lens coding model by a DF computation (DFC). This method determines entanglement entropy (EE) by implementing QDF operators in a quantum circuit. The physical link between the two system models is a quantum field lens coding algorithm (QF-LCA), which is a QF lens distance-based, implemented on real N -qubit machines. This is with the possibility to train the algorithm for making strong predictions on phase transitions as the shared objective of both models. In both system models, QDF transformations are simulated by a DFC algorithm where QDF data are collected and analyzed to represent energy states and transitions, and determine entanglement based on EE. The method gives a list of steps to simulate and optimize any thermodynamic system on macro and micro-scale observations, as presented in this article:\u2022The implementation of QF-LCA on quantum computers with EE measurement under a QDF transformation.\u2022Validation of QF-LCA as implemented compared to quantum Fourier transform (QFT) and its inverse, QFT - 1 .\u2022Quantum artificial intelligence (QAI) features by classifying QDF with strong measurement outcome predictions.", "type": "FACTOID", "question": "**QUESTION :** Which quantum field lens coding algorithm is used in the study?\n\n**CONTEXT :** This method determines entanglement entropy (EE) by implementing QDF operators in a quantum circuit. The physical link between the two system models is a quantum field lens coding algorithm (QF-LCA), which is a QF lens distance-based, implemented on real N -qubit machines."}, {"context": "This study develops a method to implement a quantum field lens coding and classification algorithm for two quantum double-field (QDF) system models: 1- a QDF model, and 2- a QDF lens coding model by a DF computation (DFC). This method determines entanglement entropy (EE) by implementing QDF operators in a quantum circuit. The physical link between the two system models is a quantum field lens coding algorithm (QF-LCA), which is a QF lens distance-based, implemented on real N -qubit machines. This is with the possibility to train the algorithm for making strong predictions on phase transitions as the shared objective of both models. In both system models, QDF transformations are simulated by a DFC algorithm where QDF data are collected and analyzed to represent energy states and transitions, and determine entanglement based on EE. The method gives a list of steps to simulate and optimize any thermodynamic system on macro and micro-scale observations, as presented in this article:\u2022The implementation of QF-LCA on quantum computers with EE measurement under a QDF transformation.\u2022Validation of QF-LCA as implemented compared to quantum Fourier transform (QFT) and its inverse, QFT - 1 .\u2022Quantum artificial intelligence (QAI) features by classifying QDF with strong measurement outcome predictions.", "type": "CASUAL", "question": "**CONTEXT :** This method determines entanglement entropy (EE) by implementing QDF operators in a quantum circuit.\n\n**QUESTION :** How does the method determine entanglement entropy?"}, {"context": "To monitor groundwater salinization due to seawater intrusion (SWI) in the aquifer of the eastern Nile Delta, Egypt, we developed a predictive regression model based on an innovative approach using SWI indicators and artificial intelligence (AI) methodologies. Hydrogeological and hydrogeochemical data of the groundwater wells in three periods (1996, 2007, and 2018) were used as input data for the AI methods. All the studied indicators were enrolled in feature extraction process where the most significant inputs were determined, including the studied year, the distance from the shoreline, the aquifer type, and the hydraulic head. These inputs were used to build four basic AI models to get the optimal prediction results of the used indicators (the base exchange index (BEX), the groundwater quality index for seawater intrusion (GQISWI), and water quality). The machine learning models utilized in this study are logistic regression, Gaussian process regression, feedforward backpropagation neural networks (FFBPN), and deep learning-based long-short-term memory. The FFBPN model achieved higher evaluation results than other models in terms of root mean square error (RMSE) and R[2] values in the testing phase, with R[2] values of 0.9667, 0.9316, and 0.9259 for BEX, GQISWI, and water quality, respectively. Accordingly, the FFBPN was used to build a predictive model for electrical conductivity for the years 2020 and 2030. Reasonable results were attained despite the imbalanced nature of the dataset for different times and sample sizes. The results show that the 1000 \u03bcS/cm boundary is expected to move inland ~9.5 km (eastern part) to ~10 km (western part) to ~12.4 km (central part) between 2018 and 2030. This encroachment would be hazardous to water resources and agriculture unless action plans are taken.", "type": "CONFIRMATIONAL", "question": "QUESTION: Did the FFBPN model achieve higher evaluation results than other models in terms of root mean square error (RMSE) and R[2] values in the testing phase?\nANSWER: Yes"}, {"context": "To monitor groundwater salinization due to seawater intrusion (SWI) in the aquifer of the eastern Nile Delta, Egypt, we developed a predictive regression model based on an innovative approach using SWI indicators and artificial intelligence (AI) methodologies. Hydrogeological and hydrogeochemical data of the groundwater wells in three periods (1996, 2007, and 2018) were used as input data for the AI methods. All the studied indicators were enrolled in feature extraction process where the most significant inputs were determined, including the studied year, the distance from the shoreline, the aquifer type, and the hydraulic head. These inputs were used to build four basic AI models to get the optimal prediction results of the used indicators (the base exchange index (BEX), the groundwater quality index for seawater intrusion (GQISWI), and water quality). The machine learning models utilized in this study are logistic regression, Gaussian process regression, feedforward backpropagation neural networks (FFBPN), and deep learning-based long-short-term memory. The FFBPN model achieved higher evaluation results than other models in terms of root mean square error (RMSE) and R[2] values in the testing phase, with R[2] values of 0.9667, 0.9316, and 0.9259 for BEX, GQISWI, and water quality, respectively. Accordingly, the FFBPN was used to build a predictive model for electrical conductivity for the years 2020 and 2030. Reasonable results were attained despite the imbalanced nature of the dataset for different times and sample sizes. The results show that the 1000 \u03bcS/cm boundary is expected to move inland ~9.5 km (eastern part) to ~10 km (western part) to ~12.4 km (central part) between 2018 and 2030. This encroachment would be hazardous to water resources and agriculture unless action plans are taken.", "type": "FACTOID", "question": "**QUESTION:** When were the hydrogeological and hydrogeochemical data of the groundwater wells used as input data for the AI methods?\n\n**CONTEXT:** Hydrogeological and hydrogeochemical data of the groundwater wells in three periods (1996, 2007, and 2018) were used as input data for the AI methods."}, {"context": "To monitor groundwater salinization due to seawater intrusion (SWI) in the aquifer of the eastern Nile Delta, Egypt, we developed a predictive regression model based on an innovative approach using SWI indicators and artificial intelligence (AI) methodologies. Hydrogeological and hydrogeochemical data of the groundwater wells in three periods (1996, 2007, and 2018) were used as input data for the AI methods. All the studied indicators were enrolled in feature extraction process where the most significant inputs were determined, including the studied year, the distance from the shoreline, the aquifer type, and the hydraulic head. These inputs were used to build four basic AI models to get the optimal prediction results of the used indicators (the base exchange index (BEX), the groundwater quality index for seawater intrusion (GQISWI), and water quality). The machine learning models utilized in this study are logistic regression, Gaussian process regression, feedforward backpropagation neural networks (FFBPN), and deep learning-based long-short-term memory. The FFBPN model achieved higher evaluation results than other models in terms of root mean square error (RMSE) and R[2] values in the testing phase, with R[2] values of 0.9667, 0.9316, and 0.9259 for BEX, GQISWI, and water quality, respectively. Accordingly, the FFBPN was used to build a predictive model for electrical conductivity for the years 2020 and 2030. Reasonable results were attained despite the imbalanced nature of the dataset for different times and sample sizes. The results show that the 1000 \u03bcS/cm boundary is expected to move inland ~9.5 km (eastern part) to ~10 km (western part) to ~12.4 km (central part) between 2018 and 2030. This encroachment would be hazardous to water resources and agriculture unless action plans are taken.", "type": "CASUAL", "question": "**CONTEXT :** The FFBPN model achieved higher evaluation results than other models in terms of root mean square error (RMSE) and R[2] values in the testing phase, with R[2] values of 0.9667, 0.9316, and 0.9259 for BEX, GQISWI, and water quality, respectively.\n\n**QUESTION :** Why did the FFBPN model achieve higher evaluation results than other models?"}, {"context": "In order to forecast the axial load-carrying capacity of concrete-filled steel tubular (CFST) columns using principal component analysis (PCA), this work compares hybrid models of artificial neural networks (ANNs) and meta-heuristic optimization algorithms (MOAs). In order to create hybrid ANN models, a dataset of 149 experimental tests was initially gathered from the accessible literature. Eight PCA-based hybrid ANNs were created using eight MOAs, including artificial bee colony, ant lion optimization, biogeography-based optimization, differential evolution, genetic algorithm, grey wolf optimizer, moth flame optimization and particle swarm optimization. The created ANNs' performance was then assessed. With R[2] ranges between 0.7094 and 0.9667 in the training phase and between 0.6883 and 0.9634 in the testing phase, we discovered that the accuracy of the built hybrid models was good. Based on the outcomes of the experiments, the generated ANN-GWO (hybrid model of ANN and grey wolf optimizer) produced the most accurate predictions in the training and testing phases, respectively, with R[2] = 0.9667 and 0.9634. The created ANN-GWO may be utilised as a substitute tool to estimate the load-carrying capacity of CFST columns in civil engineering projects according to the experimental findings.", "type": "CONFIRMATIONAL", "question": "QUESTION: Did the study compare hybrid models of artificial neural networks (ANNs) and meta-heuristic optimization algorithms (MOAs) to forecast the axial load-carrying capacity of concrete-filled steel tubular (CFST) columns using principal component analysis (PCA)?\nANSWER: Yes"}, {"context": "In order to forecast the axial load-carrying capacity of concrete-filled steel tubular (CFST) columns using principal component analysis (PCA), this work compares hybrid models of artificial neural networks (ANNs) and meta-heuristic optimization algorithms (MOAs). In order to create hybrid ANN models, a dataset of 149 experimental tests was initially gathered from the accessible literature. Eight PCA-based hybrid ANNs were created using eight MOAs, including artificial bee colony, ant lion optimization, biogeography-based optimization, differential evolution, genetic algorithm, grey wolf optimizer, moth flame optimization and particle swarm optimization. The created ANNs' performance was then assessed. With R[2] ranges between 0.7094 and 0.9667 in the training phase and between 0.6883 and 0.9634 in the testing phase, we discovered that the accuracy of the built hybrid models was good. Based on the outcomes of the experiments, the generated ANN-GWO (hybrid model of ANN and grey wolf optimizer) produced the most accurate predictions in the training and testing phases, respectively, with R[2] = 0.9667 and 0.9634. The created ANN-GWO may be utilised as a substitute tool to estimate the load-carrying capacity of CFST columns in civil engineering projects according to the experimental findings.", "type": "FACTOID", "question": "**QUESTION :** Which meta-heuristic optimization algorithm (MOA) was used in the hybrid ANN model that produced the most accurate predictions?\n\n**CONTEXT :** Based on the outcomes of the experiments, the generated ANN-GWO (hybrid model of ANN and grey wolf optimizer) produced the most accurate predictions in the training and testing phases, respectively, with R[2] = 0.9667 and 0.9634."}, {"context": "In order to forecast the axial load-carrying capacity of concrete-filled steel tubular (CFST) columns using principal component analysis (PCA), this work compares hybrid models of artificial neural networks (ANNs) and meta-heuristic optimization algorithms (MOAs). In order to create hybrid ANN models, a dataset of 149 experimental tests was initially gathered from the accessible literature. Eight PCA-based hybrid ANNs were created using eight MOAs, including artificial bee colony, ant lion optimization, biogeography-based optimization, differential evolution, genetic algorithm, grey wolf optimizer, moth flame optimization and particle swarm optimization. The created ANNs' performance was then assessed. With R[2] ranges between 0.7094 and 0.9667 in the training phase and between 0.6883 and 0.9634 in the testing phase, we discovered that the accuracy of the built hybrid models was good. Based on the outcomes of the experiments, the generated ANN-GWO (hybrid model of ANN and grey wolf optimizer) produced the most accurate predictions in the training and testing phases, respectively, with R[2] = 0.9667 and 0.9634. The created ANN-GWO may be utilised as a substitute tool to estimate the load-carrying capacity of CFST columns in civil engineering projects according to the experimental findings.", "type": "CASUAL", "question": "**CONTEXT :** The created ANNs' performance was then assessed.\n\n**QUESTION :** Why were the created ANNs' performance assessed?"}, {"context": "The prevalence of the coronavirus SARS-CoV-2 disease has resulted in the unprecedented collection of health data to support research. Historically, coordinating the collation of such datasets on a national scale has been challenging to execute for several reasons, including issues with data privacy, the lack of data reporting standards, interoperable technologies, and distribution methods. The coronavirus SARS-CoV-2 disease pandemic has highlighted the importance of collaboration between government bodies, healthcare institutions, academic researchers and commercial companies in overcoming these issues during times of urgency. The National COVID-19 Chest Imaging Database, led by NHSX, British Society of Thoracic Imaging, Royal Surrey NHS Foundation Trust and Faculty, is an example of such a national initiative. Here, we summarise the experiences and challenges of setting up the National COVID-19 Chest Imaging Database, and the implications for future ambitions of national data curation in medical imaging to advance the safe adoption of artificial intelligence in healthcare.", "type": "CONFIRMATIONAL", "question": "QUESTION: Has the coronavirus SARS-CoV-2 disease pandemic highlighted the importance of collaboration in overcoming data collection challenges?\nANSWER: Yes"}, {"context": "The prevalence of the coronavirus SARS-CoV-2 disease has resulted in the unprecedented collection of health data to support research. Historically, coordinating the collation of such datasets on a national scale has been challenging to execute for several reasons, including issues with data privacy, the lack of data reporting standards, interoperable technologies, and distribution methods. The coronavirus SARS-CoV-2 disease pandemic has highlighted the importance of collaboration between government bodies, healthcare institutions, academic researchers and commercial companies in overcoming these issues during times of urgency. The National COVID-19 Chest Imaging Database, led by NHSX, British Society of Thoracic Imaging, Royal Surrey NHS Foundation Trust and Faculty, is an example of such a national initiative. Here, we summarise the experiences and challenges of setting up the National COVID-19 Chest Imaging Database, and the implications for future ambitions of national data curation in medical imaging to advance the safe adoption of artificial intelligence in healthcare.", "type": "FACTOID", "question": "**QUESTION :** Which organisation led the National COVID-19 Chest Imaging Database?\n\n**CONTEXT :** The National COVID-19 Chest Imaging Database, led by NHSX, British Society of Thoracic Imaging, Royal Surrey NHS Foundation Trust and Faculty, is an example of such a national initiative."}, {"context": "The prevalence of the coronavirus SARS-CoV-2 disease has resulted in the unprecedented collection of health data to support research. Historically, coordinating the collation of such datasets on a national scale has been challenging to execute for several reasons, including issues with data privacy, the lack of data reporting standards, interoperable technologies, and distribution methods. The coronavirus SARS-CoV-2 disease pandemic has highlighted the importance of collaboration between government bodies, healthcare institutions, academic researchers and commercial companies in overcoming these issues during times of urgency. The National COVID-19 Chest Imaging Database, led by NHSX, British Society of Thoracic Imaging, Royal Surrey NHS Foundation Trust and Faculty, is an example of such a national initiative. Here, we summarise the experiences and challenges of setting up the National COVID-19 Chest Imaging Database, and the implications for future ambitions of national data curation in medical imaging to advance the safe adoption of artificial intelligence in healthcare.", "type": "CASUAL", "question": "**CONTEXT :** The coronavirus SARS-CoV-2 disease pandemic has highlighted the importance of collaboration between government bodies, healthcare institutions, academic researchers and commercial companies in overcoming these issues during times of urgency.\n\n**QUESTION :** Why has the coronavirus SARS-CoV-2 disease pandemic highlighted the importance of collaboration between government bodies, healthcare institutions, academic researchers and commercial companies?"}, {"context": "Parkinson's Disease (PD) is one of the most common non-curable neurodegenerative diseases. Diagnosis is achieved clinically on the basis of different symptoms with considerable delays from the onset of neurodegenerative processes in the central nervous system. In this study, we investigated early and full-blown PD patients based on the analysis of their voice characteristics with the aid of the most commonly employed machine learning (ML) techniques. A custom dataset was made with hi-fi quality recordings of vocal tasks gathered from Italian healthy control subjects and PD patients, divided into early diagnosed, off-medication patients on the one hand, and mid-advanced patients treated with L-Dopa on the other. Following the current state-of-the-art, several ML pipelines were compared usingdifferent feature selection and classification algorithms, and deep learning was also explored with a custom CNN architecture. Results show how feature-based ML and deep learning achieve comparable results in terms of classification, with KNN, SVM and na\u00efve Bayes classifiers performing similarly, with a slight edge for KNN. Much more evident is the predominance of CFS as the best feature selector. The selected features act as relevant vocal biomarkers capable of differentiating healthy subjects, early untreated PD patients and mid-advanced L-Dopa treated patients.", "type": "CONFIRMATIONAL", "question": "QUESTION: Does the study compare different machine learning techniques for classifying Parkinson's Disease patients?\nANSWER: Yes"}, {"context": "Parkinson's Disease (PD) is one of the most common non-curable neurodegenerative diseases. Diagnosis is achieved clinically on the basis of different symptoms with considerable delays from the onset of neurodegenerative processes in the central nervous system. In this study, we investigated early and full-blown PD patients based on the analysis of their voice characteristics with the aid of the most commonly employed machine learning (ML) techniques. A custom dataset was made with hi-fi quality recordings of vocal tasks gathered from Italian healthy control subjects and PD patients, divided into early diagnosed, off-medication patients on the one hand, and mid-advanced patients treated with L-Dopa on the other. Following the current state-of-the-art, several ML pipelines were compared usingdifferent feature selection and classification algorithms, and deep learning was also explored with a custom CNN architecture. Results show how feature-based ML and deep learning achieve comparable results in terms of classification, with KNN, SVM and na\u00efve Bayes classifiers performing similarly, with a slight edge for KNN. Much more evident is the predominance of CFS as the best feature selector. The selected features act as relevant vocal biomarkers capable of differentiating healthy subjects, early untreated PD patients and mid-advanced L-Dopa treated patients.", "type": "FACTOID", "question": "**QUESTION :** Which feature selector was found to be the most effective?\n\n**CONTEXT :** Much more evident is the predominance of CFS as the best feature selector."}, {"context": "Parkinson's Disease (PD) is one of the most common non-curable neurodegenerative diseases. Diagnosis is achieved clinically on the basis of different symptoms with considerable delays from the onset of neurodegenerative processes in the central nervous system. In this study, we investigated early and full-blown PD patients based on the analysis of their voice characteristics with the aid of the most commonly employed machine learning (ML) techniques. A custom dataset was made with hi-fi quality recordings of vocal tasks gathered from Italian healthy control subjects and PD patients, divided into early diagnosed, off-medication patients on the one hand, and mid-advanced patients treated with L-Dopa on the other. Following the current state-of-the-art, several ML pipelines were compared usingdifferent feature selection and classification algorithms, and deep learning was also explored with a custom CNN architecture. Results show how feature-based ML and deep learning achieve comparable results in terms of classification, with KNN, SVM and na\u00efve Bayes classifiers performing similarly, with a slight edge for KNN. Much more evident is the predominance of CFS as the best feature selector. The selected features act as relevant vocal biomarkers capable of differentiating healthy subjects, early untreated PD patients and mid-advanced L-Dopa treated patients.", "type": "CASUAL", "question": "**CONTEXT :** Diagnosis is achieved clinically on the basis of different symptoms with considerable delays from the onset of neurodegenerative processes in the central nervous system.\n\n**QUESTION :** Why is there a considerable delay in diagnosing Parkinson's Disease?"}, {"context": "The new decade has been witnessing the wide acceptance of artificial intelligence (AI) in education, followed by serious concerns about its ethics. This study examined the essence and principles of AI ethics used in education, as well as the bibliometric analysis of AI ethics for educational purposes. The clustering techniques of VOSviewer (n = 880) led the author to reveal the top 10 authors, sources, organizations, and countries in the research of AI ethics in education. The analysis of clustering solution through CitNetExplorer (n = 841) concluded that the essence of AI ethics for educational purposes included deontology, utilitarianism, and virtue, while the principles of AI ethics in education included transparency, justice, fairness, equity, non-maleficence, responsibility, and privacy. Future research could consider the influence of AI interpretability on AI ethics in education because the ability to interpret the AI decisions could help judge whether the decision is consistent with ethical criteria.", "type": "CONFIRMATIONAL", "question": "QUESTION: Does the study examine the principles of AI ethics used in education?\nANSWER: Yes"}, {"context": "The new decade has been witnessing the wide acceptance of artificial intelligence (AI) in education, followed by serious concerns about its ethics. This study examined the essence and principles of AI ethics used in education, as well as the bibliometric analysis of AI ethics for educational purposes. The clustering techniques of VOSviewer (n = 880) led the author to reveal the top 10 authors, sources, organizations, and countries in the research of AI ethics in education. The analysis of clustering solution through CitNetExplorer (n = 841) concluded that the essence of AI ethics for educational purposes included deontology, utilitarianism, and virtue, while the principles of AI ethics in education included transparency, justice, fairness, equity, non-maleficence, responsibility, and privacy. Future research could consider the influence of AI interpretability on AI ethics in education because the ability to interpret the AI decisions could help judge whether the decision is consistent with ethical criteria.", "type": "FACTOID", "question": "**QUESTION :** Which clustering techniques were used to reveal the top 10 authors, sources, organizations, and countries in the research of AI ethics in education?\n\n**CONTEXT :** The clustering techniques of VOSviewer (n = 880) led the author to reveal the top 10 authors, sources, organizations, and countries in the research of AI ethics in education."}, {"context": "The new decade has been witnessing the wide acceptance of artificial intelligence (AI) in education, followed by serious concerns about its ethics. This study examined the essence and principles of AI ethics used in education, as well as the bibliometric analysis of AI ethics for educational purposes. The clustering techniques of VOSviewer (n = 880) led the author to reveal the top 10 authors, sources, organizations, and countries in the research of AI ethics in education. The analysis of clustering solution through CitNetExplorer (n = 841) concluded that the essence of AI ethics for educational purposes included deontology, utilitarianism, and virtue, while the principles of AI ethics in education included transparency, justice, fairness, equity, non-maleficence, responsibility, and privacy. Future research could consider the influence of AI interpretability on AI ethics in education because the ability to interpret the AI decisions could help judge whether the decision is consistent with ethical criteria.", "type": "CASUAL", "question": "**CONTEXT :** The analysis of clustering solution through CitNetExplorer (n = 841) concluded that the essence of AI ethics for educational purposes included deontology, utilitarianism, and virtue, while the principles of AI ethics in education included transparency, justice, fairness, equity, non-maleficence, responsibility, and privacy.\n\n**QUESTION :** Why were deontology, utilitarianism, and virtue identified as the essence of AI ethics for educational purposes?"}, {"context": "Artificial intelligence (AI) is a broad term referring to the application of computational algorithms that can analyze large data sets to classify, predict, or gain useful conclusions. Under the umbrella of AI is machine learning (ML). ML is the process of building or learning statistical models using previously observed real world data to predict outcomes, or categorize observations based on 'training' provided by humans. These predictions are then applied to future data, all the while folding in the new data into its perpetually improving and calibrated statistical model. The future of AI and ML in healthcare research is exciting and expansive. AI and ML are becoming cornerstones in the medical and healthcare-research domains and are integral in our continued processing and capitalization of robust patient EMR data. Considerations for the use and application of ML in healthcare settings include assessing the quality of data inputs and decision-making that serve as the foundations of the ML model, ensuring the end-product is interpretable, transparent, and ethical concerns are considered throughout the development process. The current and future applications of ML include improving the quality and quantity of data collected from EMRs to improve registry data, utilizing these robust datasets to improve and standardized research protocols and outcomes, clinical decision-making applications, natural language processing and improving the fundamentals of value-based care, to name only a few.", "type": "CONFIRMATIONAL", "question": "QUESTION: Does machine learning involve building statistical models using observed data?\nANSWER: Yes"}, {"context": "Artificial intelligence (AI) is a broad term referring to the application of computational algorithms that can analyze large data sets to classify, predict, or gain useful conclusions. Under the umbrella of AI is machine learning (ML). ML is the process of building or learning statistical models using previously observed real world data to predict outcomes, or categorize observations based on 'training' provided by humans. These predictions are then applied to future data, all the while folding in the new data into its perpetually improving and calibrated statistical model. The future of AI and ML in healthcare research is exciting and expansive. AI and ML are becoming cornerstones in the medical and healthcare-research domains and are integral in our continued processing and capitalization of robust patient EMR data. Considerations for the use and application of ML in healthcare settings include assessing the quality of data inputs and decision-making that serve as the foundations of the ML model, ensuring the end-product is interpretable, transparent, and ethical concerns are considered throughout the development process. The current and future applications of ML include improving the quality and quantity of data collected from EMRs to improve registry data, utilizing these robust datasets to improve and standardized research protocols and outcomes, clinical decision-making applications, natural language processing and improving the fundamentals of value-based care, to name only a few.", "type": "FACTOID", "question": "**QUESTION :** What is the process of building or learning statistical models using previously observed real world data called?\n\n**CONTEXT :** ML is the process of building or learning statistical models using previously observed real world data to predict outcomes, or categorize observations based on 'training' provided by humans."}, {"context": "Artificial intelligence (AI) is a broad term referring to the application of computational algorithms that can analyze large data sets to classify, predict, or gain useful conclusions. Under the umbrella of AI is machine learning (ML). ML is the process of building or learning statistical models using previously observed real world data to predict outcomes, or categorize observations based on 'training' provided by humans. These predictions are then applied to future data, all the while folding in the new data into its perpetually improving and calibrated statistical model. The future of AI and ML in healthcare research is exciting and expansive. AI and ML are becoming cornerstones in the medical and healthcare-research domains and are integral in our continued processing and capitalization of robust patient EMR data. Considerations for the use and application of ML in healthcare settings include assessing the quality of data inputs and decision-making that serve as the foundations of the ML model, ensuring the end-product is interpretable, transparent, and ethical concerns are considered throughout the development process. The current and future applications of ML include improving the quality and quantity of data collected from EMRs to improve registry data, utilizing these robust datasets to improve and standardized research protocols and outcomes, clinical decision-making applications, natural language processing and improving the fundamentals of value-based care, to name only a few.", "type": "CASUAL", "question": "**CONTEXT :** AI and ML are becoming cornerstones in the medical and healthcare-research domains and are integral in our continued processing and capitalization of robust patient EMR data.\n\n**QUESTION :** Why are AI and ML becoming important in healthcare research?"}, {"context": "The difficulty of explaining the outputs of artificial intelligence (AI) models and what has led to them is a notorious ethical problem wherever these technologies are applied, including in the medical domain, and one that has no obvious solution. This paper examines the proposal, made by Luciano Floridi and colleagues, to include a new 'principle of explicability' alongside the traditional four principles of bioethics that make up the theory of 'principlism'. It specifically responds to a recent set of criticisms that challenge the supposed need for such a principle to perform an enabling role in relation to the traditional four principles and therefore suggest that these four are sufficient without the addition of explicability. The paper challenges the critics' premise that explicability cannot be an ethical principle like the classic four because it is explicitly subordinate to them. It argues instead that principlism in its original formulation locates the justification for ethical principles in a midlevel position such that they mediate between the most general moral norms and the contextual requirements of medicine. This conception of an ethical principle then provides a mold for an approach to explicability on which it functions as an enabling principle that unifies technical/epistemic demands on AI and the requirements of high-level ethical theories. The paper finishes by anticipating an objection that decision-making by clinicians and AI fall equally, but implausibly, under the principle of explicability's scope, which it rejects on the grounds that human decisions, unlike AI's, can be explained by their social environments.", "type": "CONFIRMATIONAL", "question": "QUESTION: Does the paper argue that explicability is an ethical principle like the classic four principles of bioethics?\nANSWER: Yes"}, {"context": "The difficulty of explaining the outputs of artificial intelligence (AI) models and what has led to them is a notorious ethical problem wherever these technologies are applied, including in the medical domain, and one that has no obvious solution. This paper examines the proposal, made by Luciano Floridi and colleagues, to include a new 'principle of explicability' alongside the traditional four principles of bioethics that make up the theory of 'principlism'. It specifically responds to a recent set of criticisms that challenge the supposed need for such a principle to perform an enabling role in relation to the traditional four principles and therefore suggest that these four are sufficient without the addition of explicability. The paper challenges the critics' premise that explicability cannot be an ethical principle like the classic four because it is explicitly subordinate to them. It argues instead that principlism in its original formulation locates the justification for ethical principles in a midlevel position such that they mediate between the most general moral norms and the contextual requirements of medicine. This conception of an ethical principle then provides a mold for an approach to explicability on which it functions as an enabling principle that unifies technical/epistemic demands on AI and the requirements of high-level ethical theories. The paper finishes by anticipating an objection that decision-making by clinicians and AI fall equally, but implausibly, under the principle of explicability's scope, which it rejects on the grounds that human decisions, unlike AI's, can be explained by their social environments.", "type": "FACTOID", "question": "**QUESTION :** Who proposed the inclusion of a new 'principle of explicability' alongside the traditional four principles of bioethics?\n\n**CONTEXT :** This paper examines the proposal, made by Luciano Floridi and colleagues, to include a new 'principle of explicability' alongside the traditional four principles of bioethics that make up the theory of 'principlism'."}, {"context": "The difficulty of explaining the outputs of artificial intelligence (AI) models and what has led to them is a notorious ethical problem wherever these technologies are applied, including in the medical domain, and one that has no obvious solution. This paper examines the proposal, made by Luciano Floridi and colleagues, to include a new 'principle of explicability' alongside the traditional four principles of bioethics that make up the theory of 'principlism'. It specifically responds to a recent set of criticisms that challenge the supposed need for such a principle to perform an enabling role in relation to the traditional four principles and therefore suggest that these four are sufficient without the addition of explicability. The paper challenges the critics' premise that explicability cannot be an ethical principle like the classic four because it is explicitly subordinate to them. It argues instead that principlism in its original formulation locates the justification for ethical principles in a midlevel position such that they mediate between the most general moral norms and the contextual requirements of medicine. This conception of an ethical principle then provides a mold for an approach to explicability on which it functions as an enabling principle that unifies technical/epistemic demands on AI and the requirements of high-level ethical theories. The paper finishes by anticipating an objection that decision-making by clinicians and AI fall equally, but implausibly, under the principle of explicability's scope, which it rejects on the grounds that human decisions, unlike AI's, can be explained by their social environments.", "type": "CASUAL", "question": "**CONTEXT :** This paper examines the proposal, made by Luciano Floridi and colleagues, to include a new 'principle of explicability' alongside the traditional four principles of bioethics that make up the theory of 'principlism'.\n\n**QUESTION :** Why is the principle of explicability proposed to be included alongside the traditional four principles of bioethics?"}, {"context": "Artificial intelligence applications are prevalent in the research lab and in startups, but relatively few have found their way into healthcare provider organizations. Adoption of AI innovations in consumer and business domains is typically much faster. While such delays are frustrating to those who believe in the potential of AI to transform healthcare, they are largely inherent in the structure and function of provider organizations. This article reviews the factors that govern adoption and explains why adoption has taken place at a slow pace. Research sources for the article include interviews with provider executives, healthcare IT professors and consultants, and AI vendor executives. The article considers differential speed of adoption in clinical vs. administrative applications, regulatory approval issues, reimbursement and return on investments in healthcare AI, data sources and integration with electronic health record systems, the need for clinical education, issues involving fit with clinical workflows, and ethical considerations. It concludes with a discussion of how provider organizations can successfully plan for organizational deployment.", "type": "CONFIRMATIONAL", "question": "QUESTION: Is the adoption of AI innovations in healthcare provider organizations typically faster than in consumer and business domains?\nANSWER: No"}, {"context": "Artificial intelligence applications are prevalent in the research lab and in startups, but relatively few have found their way into healthcare provider organizations. Adoption of AI innovations in consumer and business domains is typically much faster. While such delays are frustrating to those who believe in the potential of AI to transform healthcare, they are largely inherent in the structure and function of provider organizations. This article reviews the factors that govern adoption and explains why adoption has taken place at a slow pace. Research sources for the article include interviews with provider executives, healthcare IT professors and consultants, and AI vendor executives. The article considers differential speed of adoption in clinical vs. administrative applications, regulatory approval issues, reimbursement and return on investments in healthcare AI, data sources and integration with electronic health record systems, the need for clinical education, issues involving fit with clinical workflows, and ethical considerations. It concludes with a discussion of how provider organizations can successfully plan for organizational deployment.", "type": "FACTOID", "question": "**QUESTION :** Who were the research sources for the article?\n**CONTEXT :** Research sources for the article include interviews with provider executives, healthcare IT professors and consultants, and AI vendor executives."}, {"context": "Artificial intelligence applications are prevalent in the research lab and in startups, but relatively few have found their way into healthcare provider organizations. Adoption of AI innovations in consumer and business domains is typically much faster. While such delays are frustrating to those who believe in the potential of AI to transform healthcare, they are largely inherent in the structure and function of provider organizations. This article reviews the factors that govern adoption and explains why adoption has taken place at a slow pace. Research sources for the article include interviews with provider executives, healthcare IT professors and consultants, and AI vendor executives. The article considers differential speed of adoption in clinical vs. administrative applications, regulatory approval issues, reimbursement and return on investments in healthcare AI, data sources and integration with electronic health record systems, the need for clinical education, issues involving fit with clinical workflows, and ethical considerations. It concludes with a discussion of how provider organizations can successfully plan for organizational deployment.", "type": "CASUAL", "question": "**CONTEXT :** While such delays are frustrating to those who believe in the potential of AI to transform healthcare, they are largely inherent in the structure and function of provider organizations.\n\n**QUESTION :** Why has the adoption of AI innovations in healthcare provider organizations been slow?"}, {"context": "In the context that leadership matters and that leadership competencies differ from those needed to practice medicine or conduct research, developing leadership competencies for physicians is important. Indeed, effective leadership is needed ubiquitously in health care, both at the executive level and at the bedside (eg, leading clinical teams and problem-solving on the ward). Various leadership models have been proposed, most converging on common attributes, like envisioning a new and better future state, inspiring others around this shared vision, empowering others to effect the vision, modeling the expected behaviors, and engaging others by appealing to shared values. Attention to creating an organizational culture that is informed by the seven classic virtues (trust, compassion, courage, justice, wisdom, temperance, and hope) can also unleash discretionary effort in the organization to achieve high performance. Health care-specific leadership competencies include: technical expertise, not only in one's clinical/scientific arena to garner colleagues' respect but also regarding operations; strategic thinking; finance; human resources; and information technology. Also, knowledge of the regulatory and legislative environments of health care is critical, as is being a problem-solver and lifelong learner. Perhaps most important to leadership in health care, as in all sectors, is having emotional intelligence. A spectrum of leadership styles has been described, and effective leaders are facile in deploying each style in a situationally appropriate way. Overall, leadership competencies can be developed, and leadership development programs are signature features of leading health-care organizations.", "type": "CONFIRMATIONAL", "question": "QUESTION: Is emotional intelligence considered a crucial leadership competency in healthcare?\nANSWER: Yes"}, {"context": "In the context that leadership matters and that leadership competencies differ from those needed to practice medicine or conduct research, developing leadership competencies for physicians is important. Indeed, effective leadership is needed ubiquitously in health care, both at the executive level and at the bedside (eg, leading clinical teams and problem-solving on the ward). Various leadership models have been proposed, most converging on common attributes, like envisioning a new and better future state, inspiring others around this shared vision, empowering others to effect the vision, modeling the expected behaviors, and engaging others by appealing to shared values. Attention to creating an organizational culture that is informed by the seven classic virtues (trust, compassion, courage, justice, wisdom, temperance, and hope) can also unleash discretionary effort in the organization to achieve high performance. Health care-specific leadership competencies include: technical expertise, not only in one's clinical/scientific arena to garner colleagues' respect but also regarding operations; strategic thinking; finance; human resources; and information technology. Also, knowledge of the regulatory and legislative environments of health care is critical, as is being a problem-solver and lifelong learner. Perhaps most important to leadership in health care, as in all sectors, is having emotional intelligence. A spectrum of leadership styles has been described, and effective leaders are facile in deploying each style in a situationally appropriate way. Overall, leadership competencies can be developed, and leadership development programs are signature features of leading health-care organizations.", "type": "FACTOID", "question": "**QUESTION :** Which leadership competencies are specific to health care?\n\n**CONTEXT :** Health care-specific leadership competencies include: technical expertise, not only in one's clinical/scientific arena to garner colleagues' respect but also regarding operations; strategic thinking; finance; human resources; and information technology."}, {"context": "In the context that leadership matters and that leadership competencies differ from those needed to practice medicine or conduct research, developing leadership competencies for physicians is important. Indeed, effective leadership is needed ubiquitously in health care, both at the executive level and at the bedside (eg, leading clinical teams and problem-solving on the ward). Various leadership models have been proposed, most converging on common attributes, like envisioning a new and better future state, inspiring others around this shared vision, empowering others to effect the vision, modeling the expected behaviors, and engaging others by appealing to shared values. Attention to creating an organizational culture that is informed by the seven classic virtues (trust, compassion, courage, justice, wisdom, temperance, and hope) can also unleash discretionary effort in the organization to achieve high performance. Health care-specific leadership competencies include: technical expertise, not only in one's clinical/scientific arena to garner colleagues' respect but also regarding operations; strategic thinking; finance; human resources; and information technology. Also, knowledge of the regulatory and legislative environments of health care is critical, as is being a problem-solver and lifelong learner. Perhaps most important to leadership in health care, as in all sectors, is having emotional intelligence. A spectrum of leadership styles has been described, and effective leaders are facile in deploying each style in a situationally appropriate way. Overall, leadership competencies can be developed, and leadership development programs are signature features of leading health-care organizations.", "type": "CASUAL", "question": "CONTEXT :  Attention to creating an organizational culture that is informed by the seven classic virtues (trust, compassion, courage, justice, wisdom, temperance, and hope) can also unleash discretionary effort in the organization to achieve high performance.\n\nQUESTION : Why is it important to create an organizational culture informed by the seven classic virtues?"}, {"context": "['Artificial intelligence (AI) comprises computational models that mimic the human brain to perform various diagnostic tasks in clinical practice. The aim of this scoping review was to systematically analyze the AI algorithms and models used in endodontics and identify the source quality and type of evidence.', 'A literature search was conducted in October 2020 to identify the relevant literature in English language in the 4 major health sciences databases, ie, MEDLINE, Dentistry  Oral Science, CINAHL Plus, and Cochrane Library. Our review questions were the following: what are the different AI algorithms and models used in endodontics?, what are the datasets being used?, what type of performance metrics were reported?, and what diagnostic performance measures were used?. The quality of the included studies was evaluated by a modified Quality Assessment of Studies of Diagnostic Accuracy risk (QUADAS) tool.', 'Out of 300 studies, 12 articles met our inclusion criteria and were subjected to final analysis. Among the included studies, 6 studies focused on periapical pathology, and 3 studies investigated vertical root fractures. Most studies (n\\xa0=\\xa010) used neural networks, among which convolutional neural networks were commonly used. The datasets that were mostly studied were radiographs. Out of 12 studies, only 3 studies achieved a high score according to the modified QUADAS tool.', 'AI models had acceptable performance, ie, accuracy 90% in executing various diagnostic tasks. The scientific reporting of AI-related research is irregular. The endodontic community needs to implement recommended guidelines to improve the weaknesses in the current planning and reporting of AI-related research to improve its scientific vigor.']", "type": "CONFIRMATIONAL", "question": "QUESTION: Did the literature search include studies published in languages other than English?\nANSWER: No"}, {"context": "['Artificial intelligence (AI) comprises computational models that mimic the human brain to perform various diagnostic tasks in clinical practice. The aim of this scoping review was to systematically analyze the AI algorithms and models used in endodontics and identify the source quality and type of evidence.', 'A literature search was conducted in October 2020 to identify the relevant literature in English language in the 4 major health sciences databases, ie, MEDLINE, Dentistry  Oral Science, CINAHL Plus, and Cochrane Library. Our review questions were the following: what are the different AI algorithms and models used in endodontics?, what are the datasets being used?, what type of performance metrics were reported?, and what diagnostic performance measures were used?. The quality of the included studies was evaluated by a modified Quality Assessment of Studies of Diagnostic Accuracy risk (QUADAS) tool.', 'Out of 300 studies, 12 articles met our inclusion criteria and were subjected to final analysis. Among the included studies, 6 studies focused on periapical pathology, and 3 studies investigated vertical root fractures. Most studies (n\\xa0=\\xa010) used neural networks, among which convolutional neural networks were commonly used. The datasets that were mostly studied were radiographs. Out of 12 studies, only 3 studies achieved a high score according to the modified QUADAS tool.', 'AI models had acceptable performance, ie, accuracy 90% in executing various diagnostic tasks. The scientific reporting of AI-related research is irregular. The endodontic community needs to implement recommended guidelines to improve the weaknesses in the current planning and reporting of AI-related research to improve its scientific vigor.']", "type": "FACTOID", "question": "**QUESTION :** When was the literature search conducted?\n\n**CONTEXT :** A literature search was conducted in October 2020 to identify the relevant literature in English language in the 4 major health sciences databases, ie, MEDLINE, Dentistry  Oral Science, CINAHL Plus, and Cochrane Library."}, {"context": "['Artificial intelligence (AI) comprises computational models that mimic the human brain to perform various diagnostic tasks in clinical practice. The aim of this scoping review was to systematically analyze the AI algorithms and models used in endodontics and identify the source quality and type of evidence.', 'A literature search was conducted in October 2020 to identify the relevant literature in English language in the 4 major health sciences databases, ie, MEDLINE, Dentistry  Oral Science, CINAHL Plus, and Cochrane Library. Our review questions were the following: what are the different AI algorithms and models used in endodontics?, what are the datasets being used?, what type of performance metrics were reported?, and what diagnostic performance measures were used?. The quality of the included studies was evaluated by a modified Quality Assessment of Studies of Diagnostic Accuracy risk (QUADAS) tool.', 'Out of 300 studies, 12 articles met our inclusion criteria and were subjected to final analysis. Among the included studies, 6 studies focused on periapical pathology, and 3 studies investigated vertical root fractures. Most studies (n\\xa0=\\xa010) used neural networks, among which convolutional neural networks were commonly used. The datasets that were mostly studied were radiographs. Out of 12 studies, only 3 studies achieved a high score according to the modified QUADAS tool.', 'AI models had acceptable performance, ie, accuracy 90% in executing various diagnostic tasks. The scientific reporting of AI-related research is irregular. The endodontic community needs to implement recommended guidelines to improve the weaknesses in the current planning and reporting of AI-related research to improve its scientific vigor.']", "type": "CASUAL", "question": "**CONTEXT :** \"The scientific reporting of AI-related research is irregular.\"\n\n**QUESTION :** Why is the scientific reporting of AI-related research irregular?"}, {"context": "The Artificial Intelligence and Augmented Intelligence for Automated Investigation for Scientific Discovery Network[+] (AI3SD) was established in response to the UK Engineering and Physical Sciences Research Council (EPSRC) late-2017 call for a Network[+] to promote cutting-edge research in artificial intelligence to accelerate groundbreaking scientific discoveries. This article provides the philosophical, scientific, and technical underpinnings of the Network[+], the history of the different domains represented in the Network[+], and the specific focus of the Network[+]. The activities, collaborations, and research covered in the first year of the Network[+] have highlighted the significant challenges in the chemistry and augmented and artificial intelligence space. These challenges are shaping the future directions of the Network[+]. The article concludes with a summary of the lessons learned in running this Network[+] and introduces our plans for the future in a landscape redrawn by COVID-19, including rebranding into the AI 4 Scientific Discovery Network (www.ai4science.network).", "type": "CONFIRMATIONAL", "question": "QUESTION: Was the AI3SD Network established in response to a call from the UK Engineering and Physical Sciences Research Council (EPSRC)?\nANSWER: Yes"}, {"context": "The Artificial Intelligence and Augmented Intelligence for Automated Investigation for Scientific Discovery Network[+] (AI3SD) was established in response to the UK Engineering and Physical Sciences Research Council (EPSRC) late-2017 call for a Network[+] to promote cutting-edge research in artificial intelligence to accelerate groundbreaking scientific discoveries. This article provides the philosophical, scientific, and technical underpinnings of the Network[+], the history of the different domains represented in the Network[+], and the specific focus of the Network[+]. The activities, collaborations, and research covered in the first year of the Network[+] have highlighted the significant challenges in the chemistry and augmented and artificial intelligence space. These challenges are shaping the future directions of the Network[+]. The article concludes with a summary of the lessons learned in running this Network[+] and introduces our plans for the future in a landscape redrawn by COVID-19, including rebranding into the AI 4 Scientific Discovery Network (www.ai4science.network).", "type": "FACTOID", "question": "**QUESTION :** When was the AI3SD Network established?\n\n**CONTEXT :** The Artificial Intelligence and Augmented Intelligence for Automated Investigation for Scientific Discovery Network[+] (AI3SD) was established in response to the UK Engineering and Physical Sciences Research Council (EPSRC) late-2017 call for a Network[+] to promote cutting-edge research in artificial intelligence to accelerate groundbreaking scientific discoveries."}, {"context": "The Artificial Intelligence and Augmented Intelligence for Automated Investigation for Scientific Discovery Network[+] (AI3SD) was established in response to the UK Engineering and Physical Sciences Research Council (EPSRC) late-2017 call for a Network[+] to promote cutting-edge research in artificial intelligence to accelerate groundbreaking scientific discoveries. This article provides the philosophical, scientific, and technical underpinnings of the Network[+], the history of the different domains represented in the Network[+], and the specific focus of the Network[+]. The activities, collaborations, and research covered in the first year of the Network[+] have highlighted the significant challenges in the chemistry and augmented and artificial intelligence space. These challenges are shaping the future directions of the Network[+]. The article concludes with a summary of the lessons learned in running this Network[+] and introduces our plans for the future in a landscape redrawn by COVID-19, including rebranding into the AI 4 Scientific Discovery Network (www.ai4science.network).", "type": "CASUAL", "question": "**CONTEXT :** The Artificial Intelligence and Augmented Intelligence for Automated Investigation for Scientific Discovery Network[+] (AI3SD) was established in response to the UK Engineering and Physical Sciences Research Council (EPSRC) late-2017 call for a Network[+] to promote cutting-edge research in artificial intelligence to accelerate groundbreaking scientific discoveries.\n\n**QUESTION :** Why was the AI3SD Network established?"}, {"context": "Die in den letzten Jahren vorangetriebene standardisierte, strukturierte, radiologische Befundung (SSRB) verfolgt mehrere Ziele: Die Befunde sollen vollst\u00e4ndig, eindeutig, verst\u00e4ndlich und stringent sein; Wiederholungen oder \u00fcberfl\u00fcssige Inhalte sollen vermieden werden. Dar\u00fcber hinaus ergeben sich Vorteile bei der Darstellung zeitlicher Verl\u00e4ufe, Nachverfolgungen und Korrelationen mit strukturierten Befunden anderer Disziplinen sowie die Nutzung KI-basierter (k\u00fcnstliche Intelligenz) Methoden. Die Erstellung der vorliegenden Befundvorlage (\u201eTemplate\u201c) f\u00fcr die SSRB von nativen Computertomographien (CT) in der Harnsteindiagnostik folgte dem von der Deutschen R\u00f6ntgengesellschaft (DRG) vorgeschlagenen \u201eProzess zur Erstellung qualit\u00e4tsgesicherter und konsensbasierter Befundvorlagen sowie anschlie\u00dfender kontinuierlicher Qualit\u00e4tskontrolle und Aktualisierung\u201c mit mehreren Stufen von Entw\u00fcrfen, Konsensusmeetings und Weiterentwicklungen. Die finale Version wurde auf der Website der DRG (www.befundung.drg.de) ver\u00f6ffentlicht. Das Template soll von der Steuerungsgruppe j\u00e4hrlich \u00fcberpr\u00fcft und ggf. angepasst werden. Die Befundvorlage beinhaltet 6\u00a0Organdom\u00e4nen (z.\u202fB. rechte Niere) f\u00fcr die Eingaben f\u00fcr insgesamt 21\u00a0verschiedene Items, die meistens mit Auswahlfenstern gemacht werden k\u00f6nnen. Wird f\u00fcr ein Organ bei der ersten Abfrage \u201ekein Nachweis von Steinen\u201c gew\u00e4hlt, springt die Abfrage automatisch zum n\u00e4chsten Organ, so dass die Bearbeitung trotz der potenziell hohen Gesamtzahl an Einzelabfragen zu allen Organen sehr z\u00fcgig zu bearbeiten ist. Die Etablierung einer standardisierten strukturierten Befundung von Schnittbildgebungsverfahren nicht nur der onkologischen Radiologie wird von den deutschen, europ\u00e4ischen und nordamerikanischen radiologischen Gesellschaften als eine der aktuellen zentralen Aufgabe wahrgenommen. Mit der vorliegenden Befundvorlage zur Beschreibung von CT-Befunden zur Harnsteindiagnostik stellen wir die erste Version einer urologischen Vorlage \u00fcberhaupt vor. Weitere Templates f\u00fcr urologische Fragestellungen sollen folgen.", "type": "CONFIRMATIONAL", "question": "QUESTION: Ist die Befundvorlage f\u00fcr die SSRB von nativen Computertomographien (CT) in der Harnsteindiagnostik auf der Website der DRG ver\u00f6ffentlicht?\nANSWER: Ja"}, {"context": "Die in den letzten Jahren vorangetriebene standardisierte, strukturierte, radiologische Befundung (SSRB) verfolgt mehrere Ziele: Die Befunde sollen vollst\u00e4ndig, eindeutig, verst\u00e4ndlich und stringent sein; Wiederholungen oder \u00fcberfl\u00fcssige Inhalte sollen vermieden werden. Dar\u00fcber hinaus ergeben sich Vorteile bei der Darstellung zeitlicher Verl\u00e4ufe, Nachverfolgungen und Korrelationen mit strukturierten Befunden anderer Disziplinen sowie die Nutzung KI-basierter (k\u00fcnstliche Intelligenz) Methoden. Die Erstellung der vorliegenden Befundvorlage (\u201eTemplate\u201c) f\u00fcr die SSRB von nativen Computertomographien (CT) in der Harnsteindiagnostik folgte dem von der Deutschen R\u00f6ntgengesellschaft (DRG) vorgeschlagenen \u201eProzess zur Erstellung qualit\u00e4tsgesicherter und konsensbasierter Befundvorlagen sowie anschlie\u00dfender kontinuierlicher Qualit\u00e4tskontrolle und Aktualisierung\u201c mit mehreren Stufen von Entw\u00fcrfen, Konsensusmeetings und Weiterentwicklungen. Die finale Version wurde auf der Website der DRG (www.befundung.drg.de) ver\u00f6ffentlicht. Das Template soll von der Steuerungsgruppe j\u00e4hrlich \u00fcberpr\u00fcft und ggf. angepasst werden. Die Befundvorlage beinhaltet 6\u00a0Organdom\u00e4nen (z.\u202fB. rechte Niere) f\u00fcr die Eingaben f\u00fcr insgesamt 21\u00a0verschiedene Items, die meistens mit Auswahlfenstern gemacht werden k\u00f6nnen. Wird f\u00fcr ein Organ bei der ersten Abfrage \u201ekein Nachweis von Steinen\u201c gew\u00e4hlt, springt die Abfrage automatisch zum n\u00e4chsten Organ, so dass die Bearbeitung trotz der potenziell hohen Gesamtzahl an Einzelabfragen zu allen Organen sehr z\u00fcgig zu bearbeiten ist. Die Etablierung einer standardisierten strukturierten Befundung von Schnittbildgebungsverfahren nicht nur der onkologischen Radiologie wird von den deutschen, europ\u00e4ischen und nordamerikanischen radiologischen Gesellschaften als eine der aktuellen zentralen Aufgabe wahrgenommen. Mit der vorliegenden Befundvorlage zur Beschreibung von CT-Befunden zur Harnsteindiagnostik stellen wir die erste Version einer urologischen Vorlage \u00fcberhaupt vor. Weitere Templates f\u00fcr urologische Fragestellungen sollen folgen.", "type": "FACTOID", "question": "**QUESTION:** When was the final version of the Befundvorlage published?\n\n**CONTEXT:** Die finale Version wurde auf der Website der DRG (www.befundung.drg.de) ver\u00f6ffentlicht."}, {"context": "Die in den letzten Jahren vorangetriebene standardisierte, strukturierte, radiologische Befundung (SSRB) verfolgt mehrere Ziele: Die Befunde sollen vollst\u00e4ndig, eindeutig, verst\u00e4ndlich und stringent sein; Wiederholungen oder \u00fcberfl\u00fcssige Inhalte sollen vermieden werden. Dar\u00fcber hinaus ergeben sich Vorteile bei der Darstellung zeitlicher Verl\u00e4ufe, Nachverfolgungen und Korrelationen mit strukturierten Befunden anderer Disziplinen sowie die Nutzung KI-basierter (k\u00fcnstliche Intelligenz) Methoden. Die Erstellung der vorliegenden Befundvorlage (\u201eTemplate\u201c) f\u00fcr die SSRB von nativen Computertomographien (CT) in der Harnsteindiagnostik folgte dem von der Deutschen R\u00f6ntgengesellschaft (DRG) vorgeschlagenen \u201eProzess zur Erstellung qualit\u00e4tsgesicherter und konsensbasierter Befundvorlagen sowie anschlie\u00dfender kontinuierlicher Qualit\u00e4tskontrolle und Aktualisierung\u201c mit mehreren Stufen von Entw\u00fcrfen, Konsensusmeetings und Weiterentwicklungen. Die finale Version wurde auf der Website der DRG (www.befundung.drg.de) ver\u00f6ffentlicht. Das Template soll von der Steuerungsgruppe j\u00e4hrlich \u00fcberpr\u00fcft und ggf. angepasst werden. Die Befundvorlage beinhaltet 6\u00a0Organdom\u00e4nen (z.\u202fB. rechte Niere) f\u00fcr die Eingaben f\u00fcr insgesamt 21\u00a0verschiedene Items, die meistens mit Auswahlfenstern gemacht werden k\u00f6nnen. Wird f\u00fcr ein Organ bei der ersten Abfrage \u201ekein Nachweis von Steinen\u201c gew\u00e4hlt, springt die Abfrage automatisch zum n\u00e4chsten Organ, so dass die Bearbeitung trotz der potenziell hohen Gesamtzahl an Einzelabfragen zu allen Organen sehr z\u00fcgig zu bearbeiten ist. Die Etablierung einer standardisierten strukturierten Befundung von Schnittbildgebungsverfahren nicht nur der onkologischen Radiologie wird von den deutschen, europ\u00e4ischen und nordamerikanischen radiologischen Gesellschaften als eine der aktuellen zentralen Aufgabe wahrgenommen. Mit der vorliegenden Befundvorlage zur Beschreibung von CT-Befunden zur Harnsteindiagnostik stellen wir die erste Version einer urologischen Vorlage \u00fcberhaupt vor. Weitere Templates f\u00fcr urologische Fragestellungen sollen folgen.", "type": "CASUAL", "question": "CONTEXT : Die Erstellung der vorliegenden Befundvorlage (\u201eTemplate\u201c) f\u00fcr die SSRB von nativen Computertomographien (CT) in der Harnsteindiagnostik folgte dem von der Deutschen R\u00f6ntgengesellschaft (DRG) vorgeschlagenen \u201eProzess zur Erstellung qualit\u00e4tsgesicherter und konsensbasierter Befundvorlagen sowie anschlie\u00dfender kontinuierlicher Qualit\u00e4tskontrolle und Aktualisierung\u201c mit mehreren Stufen von Entw\u00fcrfen, Konsensusmeetings und Weiterentwicklungen.\n\nQUESTION : Warum wurde die Befundvorlage f\u00fcr die SSRB von nativen Computertomographien (CT) in der Harnsteindiagnostik nach dem von der Deutschen R\u00f6ntgengesellschaft (DRG) vorgeschlagenen Prozess erstellt?"}, {"context": "The objective of the present study was to analyze the extent to which violent peer behavior and victimization, both traditional and cybernetic, and predict certain indicators of psychological maladjustment in adolescents, such as self-concept, satisfaction with life, feeling of loneliness, depressive symptomatology, perceived stress, social anxiety, empathy, and emotional intelligence. Participants in the study were 1318 adolescents of both sexes, aged between 11 and 18 years and enrolled in Compulsory Secondary Education schools. The design of the study was cross-sectional. The results indicated that the victims generally present greater maladjustment than the aggressors. Both victims and cybervictims showed a greater decrease in all the dimensions of self-concept, compared with aggressors and cyberaggressors. However, the two types of aggressors showed a higher likelihood of presenting low levels of empathy. Feeling of loneliness, depressive symptomatology, perceived stress, and degree of life satisfaction was more probable to be present in all groups of aggressors and victims. Finally, with regard to emotional intelligence, victims had a higher probability of obtaining low scores in all the dimensions of this construct; this was the case for traditional aggressors only in the dimension of emotion regulation. These results contribute to our understanding of the consequences of harassment in the adaptation of the students involved, with relevant practical implications.", "type": "CONFIRMATIONAL", "question": "QUESTION: Do the results of the study indicate that victims of cyberbullying experience greater psychological maladjustment than traditional aggressors?\nANSWER: Yes"}, {"context": "The objective of the present study was to analyze the extent to which violent peer behavior and victimization, both traditional and cybernetic, and predict certain indicators of psychological maladjustment in adolescents, such as self-concept, satisfaction with life, feeling of loneliness, depressive symptomatology, perceived stress, social anxiety, empathy, and emotional intelligence. Participants in the study were 1318 adolescents of both sexes, aged between 11 and 18 years and enrolled in Compulsory Secondary Education schools. The design of the study was cross-sectional. The results indicated that the victims generally present greater maladjustment than the aggressors. Both victims and cybervictims showed a greater decrease in all the dimensions of self-concept, compared with aggressors and cyberaggressors. However, the two types of aggressors showed a higher likelihood of presenting low levels of empathy. Feeling of loneliness, depressive symptomatology, perceived stress, and degree of life satisfaction was more probable to be present in all groups of aggressors and victims. Finally, with regard to emotional intelligence, victims had a higher probability of obtaining low scores in all the dimensions of this construct; this was the case for traditional aggressors only in the dimension of emotion regulation. These results contribute to our understanding of the consequences of harassment in the adaptation of the students involved, with relevant practical implications.", "type": "FACTOID", "question": "**QUESTION :** How many participants were involved in the study?\n\n**CONTEXT :** Participants in the study were 1318 adolescents of both sexes, aged between 11 and 18 years and enrolled in Compulsory Secondary Education schools."}, {"context": "The objective of the present study was to analyze the extent to which violent peer behavior and victimization, both traditional and cybernetic, and predict certain indicators of psychological maladjustment in adolescents, such as self-concept, satisfaction with life, feeling of loneliness, depressive symptomatology, perceived stress, social anxiety, empathy, and emotional intelligence. Participants in the study were 1318 adolescents of both sexes, aged between 11 and 18 years and enrolled in Compulsory Secondary Education schools. The design of the study was cross-sectional. The results indicated that the victims generally present greater maladjustment than the aggressors. Both victims and cybervictims showed a greater decrease in all the dimensions of self-concept, compared with aggressors and cyberaggressors. However, the two types of aggressors showed a higher likelihood of presenting low levels of empathy. Feeling of loneliness, depressive symptomatology, perceived stress, and degree of life satisfaction was more probable to be present in all groups of aggressors and victims. Finally, with regard to emotional intelligence, victims had a higher probability of obtaining low scores in all the dimensions of this construct; this was the case for traditional aggressors only in the dimension of emotion regulation. These results contribute to our understanding of the consequences of harassment in the adaptation of the students involved, with relevant practical implications.", "type": "CASUAL", "question": "CONTEXT : The results indicated that the victims generally present greater maladjustment than the aggressors.\n\nQUESTION : Why do victims generally present greater maladjustment than the aggressors?"}, {"context": "Machine learning (ML) offers robust statistical and probabilistic techniques that can help to make sense of large amounts of data. This scoping review paper aims to broadly explore the nature of research activity using ML in the context of psychological talk therapies, highlighting the scope of current methods and considerations for clinical practice and directions for future research. Using a systematic search methodology, fifty-one studies were identified. A narrative synthesis indicates two types of studies, those who developed and tested an ML model (k=44), and those who reported on the feasibility of a particular treatment tool that uses an ML algorithm (k=7). Most model development studies used supervised learning techniques to classify or predict labeled treatment process or outcome data, whereas others used unsupervised techniques to identify clusters in the unlabeled patient or treatment data. Overall, the current applications of ML in psychotherapy research demonstrated a range of possible benefits for indications of treatment process, adherence, therapist skills and treatment response prediction, as well as ways to accelerate research through automated behavioral or linguistic process coding. Given the novelty and potential of this research field, these proof-of-concept studies are encouraging, however, do not necessarily translate to improved clinical practice (yet).", "type": "CONFIRMATIONAL", "question": "QUESTION: Does the text suggest that the current applications of ML in psychotherapy research have been proven to improve clinical practice?\nANSWER: No"}, {"context": "Machine learning (ML) offers robust statistical and probabilistic techniques that can help to make sense of large amounts of data. This scoping review paper aims to broadly explore the nature of research activity using ML in the context of psychological talk therapies, highlighting the scope of current methods and considerations for clinical practice and directions for future research. Using a systematic search methodology, fifty-one studies were identified. A narrative synthesis indicates two types of studies, those who developed and tested an ML model (k=44), and those who reported on the feasibility of a particular treatment tool that uses an ML algorithm (k=7). Most model development studies used supervised learning techniques to classify or predict labeled treatment process or outcome data, whereas others used unsupervised techniques to identify clusters in the unlabeled patient or treatment data. Overall, the current applications of ML in psychotherapy research demonstrated a range of possible benefits for indications of treatment process, adherence, therapist skills and treatment response prediction, as well as ways to accelerate research through automated behavioral or linguistic process coding. Given the novelty and potential of this research field, these proof-of-concept studies are encouraging, however, do not necessarily translate to improved clinical practice (yet).", "type": "FACTOID", "question": "**QUESTION :** Which types of studies were identified in the narrative synthesis?\n\n**CONTEXT :** A narrative synthesis indicates two types of studies, those who developed and tested an ML model (k=44), and those who reported on the feasibility of a particular treatment tool that uses an ML algorithm (k=7)."}, {"context": "Machine learning (ML) offers robust statistical and probabilistic techniques that can help to make sense of large amounts of data. This scoping review paper aims to broadly explore the nature of research activity using ML in the context of psychological talk therapies, highlighting the scope of current methods and considerations for clinical practice and directions for future research. Using a systematic search methodology, fifty-one studies were identified. A narrative synthesis indicates two types of studies, those who developed and tested an ML model (k=44), and those who reported on the feasibility of a particular treatment tool that uses an ML algorithm (k=7). Most model development studies used supervised learning techniques to classify or predict labeled treatment process or outcome data, whereas others used unsupervised techniques to identify clusters in the unlabeled patient or treatment data. Overall, the current applications of ML in psychotherapy research demonstrated a range of possible benefits for indications of treatment process, adherence, therapist skills and treatment response prediction, as well as ways to accelerate research through automated behavioral or linguistic process coding. Given the novelty and potential of this research field, these proof-of-concept studies are encouraging, however, do not necessarily translate to improved clinical practice (yet).", "type": "CASUAL", "question": "**CONTEXT :** Given the novelty and potential of this research field, these proof-of-concept studies are encouraging, however, do not necessarily translate to improved clinical practice (yet).\n\n**QUESTION :** Why do proof-of-concept studies in machine learning for psychotherapy research not necessarily translate to improved clinical practice?"}]